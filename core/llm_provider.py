"""LLMæä¾›å•†æ¨¡å—

ä¸ºè®ºæ–‡åˆ†æå’Œæ€»ç»“æä¾›OpenAIå…¼å®¹APIé›†æˆï¼Œæ”¯æŒé€šä¹‰åƒé—®ã€SiliconFlowã€OLLAMAç­‰ã€‚
åŒæ—¶åŒ…å«LLMæä¾›å•†çš„æŠ½è±¡åŸºç±»å®šä¹‰ã€‚
"""

import time
import json
import traceback
import os
from openai import OpenAI
import threading
from typing import Optional, Dict, Any, List
from loguru import logger


class LLMProvider:
    """ç”¨äºLLMäº¤äº’çš„é€šç”¨APIæä¾›å•†ï¼Œæ”¯æŒé€šä¹‰åƒé—®ã€SiliconFlowç­‰OpenAIå…¼å®¹APIã€‚
    è´Ÿè´£æ‰€æœ‰LLMæç¤ºè¯æ„å»ºå’Œäº¤äº’é€»è¾‘ã€‚"""
    
    def __init__(self, model: str, base_url: str, api_key: str, description: str = "", username: str = "TEST", 
                 temperature: float = 0.7, top_p: float = 0.9, max_tokens: int = 4000):
        """åˆå§‹åŒ–LLMæä¾›å•†ã€‚
        
        Args:
            model: æ¨¡å‹åç§°
            base_url: APIåŸºç¡€URL
            api_key: APIå¯†é’¥
            description: ç ”ç©¶å…´è¶£æè¿°
            username: ç”¨æˆ·åï¼Œç”¨äºç”ŸæˆæŠ¥å‘Šæ—¶çš„ç½²å
            temperature: é»˜è®¤æ¸©åº¦å‚æ•°
            top_p: é»˜è®¤top_på‚æ•°
            max_tokens: é»˜è®¤æœ€å¤§tokenæ•°
        """
        logger.info(f"LLMProvideråˆå§‹åŒ–å¼€å§‹")
        self._model_name = model
        self._client = OpenAI(base_url=base_url, api_key=api_key)
        self.description = description
        self.username = username
        self.default_temperature = temperature
        self.default_top_p = top_p
        self.default_max_tokens = max_tokens
        # Tokenç”¨é‡ç»Ÿè®¡ï¼ˆä½œä¸ºå•ä¸€çœŸæºï¼‰
        self.total_input_tokens = 0
        self.total_output_tokens = 0
        self.total_tokens = 0
        # å¹¶å‘é™æµï¼ˆç»Ÿä¸€å…¥å£ï¼Œç±»çº§å…±äº«ä¿¡å·é‡ï¼Œè·¨å®ä¾‹ç»Ÿä¸€é™æµï¼‰
        try:
            max_concurrency = int(os.getenv('LLM_MAX_CONCURRENCY', '2'))
            if max_concurrency < 1:
                max_concurrency = 1
        except Exception:
            max_concurrency = 2
        self._max_concurrency = max_concurrency
        if not hasattr(LLMProvider, "_global_rate_limiter") or LLMProvider._global_rate_limiter is None:
            LLMProvider._global_rate_limiter = threading.BoundedSemaphore(self._max_concurrency)
        self._rate_limiter = LLMProvider._global_rate_limiter
        logger.success(f"LLMProvideråˆå§‹åŒ–å®Œæˆ - æ¨¡å‹: {model}, URL: {base_url}, ç”¨æˆ·: {username}, æ¸©åº¦: {temperature}, top_p: {top_p}, max_tokens: {max_tokens}")
    
    @property
    def model_name(self) -> str:
        """è·å–æ¨¡å‹åç§°ã€‚
        
        Returns:
            æ¨¡å‹åç§°å­—ç¬¦ä¸²
        """
        return self._model_name
    
    def _build_messages(self, prompt: str) -> list:
        """æ„å»ºOpenAI APIçš„æ¶ˆæ¯ç»“æ„ã€‚
        
        Args:
            prompt: ç”¨æˆ·æç¤ºæ–‡æœ¬
            
        Returns:
            æ¶ˆæ¯åˆ—è¡¨
        """
        return [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": prompt,
                    }
                ]
            }
        ]
    
    def _call_api_with_retry(
        self, messages: list, temperature: float = None, top_p: float = None, 
        max_tokens: int = None, max_retries: int = 2, wait_time: int = 1, return_raw: bool = False
    ) -> str:
        """ä½¿ç”¨é‡è¯•æœºåˆ¶è°ƒç”¨OpenAI APIã€‚
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: ç”Ÿæˆæ¸©åº¦ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
            top_p: top_på‚æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
            max_tokens: æœ€å¤§tokenæ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            wait_time: é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            APIå“åº”å†…å®¹
            
        Raises:
            Exception: APIè°ƒç”¨å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        # ä½¿ç”¨é»˜è®¤å€¼å¦‚æœå‚æ•°ä¸ºNone
        if temperature is None:
            temperature = self.default_temperature
        if top_p is None:
            top_p = self.default_top_p
        if max_tokens is None:
            max_tokens = self.default_max_tokens
            
        logger.debug(f"APIè°ƒç”¨å¼€å§‹ - æ¨¡å‹: {self._model_name}, æ¸©åº¦: {temperature}, top_p: {top_p}, max_tokens: {max_tokens}, æœ€å¤§é‡è¯•: {max_retries}")
        logger.debug(f"APIé…ç½® - å®¢æˆ·ç«¯: {self._client}, åŸºç¡€URL: {self._client.base_url}")
        
        for attempt in range(max_retries):
            try:
                # å…¨å±€å¹¶å‘é™æµ
                self._rate_limiter.acquire()
                logger.debug(f"ç¬¬ {attempt + 1} æ¬¡APIè°ƒç”¨å°è¯•")
                response = self._client.chat.completions.create(
                    model=self._model_name,
                    messages=messages,
                    temperature=temperature,
                    top_p=top_p,
                    max_tokens=max_tokens,
                )
                logger.debug(f"APIè°ƒç”¨æˆåŠŸ - å°è¯•æ¬¡æ•°: {attempt + 1}")
                # æ›´æ–°tokenç»Ÿè®¡ï¼ˆå…¼å®¹æ— usageåœºæ™¯ï¼‰
                try:
                    usage = getattr(response, 'usage', None)
                    if usage:
                        self.total_input_tokens += getattr(usage, 'prompt_tokens', 0) or 0
                        self.total_output_tokens += getattr(usage, 'completion_tokens', 0) or 0
                        self.total_tokens += getattr(usage, 'total_tokens', 0) or 0
                except Exception:
                    pass
                if return_raw:
                    return response
                return response.choices[0].message.content
                
            except Exception as error:
                error_str = str(error).lower()
                error_type = type(error).__name__
                
                # è¯¦ç»†è®°å½•é”™è¯¯ä¿¡æ¯
                logger.error(f"APIè°ƒç”¨é”™è¯¯è¯¦æƒ…:")
                logger.error(f"  - é”™è¯¯ç±»å‹: {error_type}")
                logger.error(f"  - é”™è¯¯æ¶ˆæ¯: {error}")
                logger.error(f"  - æ¨¡å‹åç§°: {self._model_name}")
                logger.error(f"  - åŸºç¡€URL: {self._client.base_url}")
                logger.error(f"  - å°è¯•æ¬¡æ•°: {attempt + 1}/{max_retries}")
                
                # æ ¹æ®é”™è¯¯ç±»å‹å†³å®šé‡è¯•ç­–ç•¥
                if any(keyword in error_str for keyword in ['rate_limit', '429', 'quota', 'limit']):
                    # APIé™æµé”™è¯¯ï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿
                    wait_time = (attempt + 1) * 3
                    logger.warning(f"APIé™æµ ({attempt + 1}/{max_retries}) - {error}")
                elif any(keyword in error_str for keyword in ['timeout', 'connection', 'network']):
                    # ç½‘ç»œé”™è¯¯ï¼Œçº¿æ€§é€€é¿
                    wait_time = (attempt + 1) * 2
                    logger.warning(f"ç½‘ç»œé”™è¯¯ ({attempt + 1}/{max_retries}) - {error}")
                elif any(keyword in error_str for keyword in ['unauthorized', '401', 'api_key', 'authentication']):
                    # è®¤è¯é”™è¯¯ï¼Œä¸é‡è¯•
                    logger.error(f"APIè®¤è¯é”™è¯¯ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥é…ç½®: {error}")
                    raise
                elif any(keyword in error_str for keyword in ['not found', '404', 'model']):
                    # æ¨¡å‹ä¸å­˜åœ¨é”™è¯¯ï¼Œä¸é‡è¯•
                    logger.error(f"æ¨¡å‹ä¸å­˜åœ¨æˆ–ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥æ¨¡å‹åç§°: {error}")
                    raise
                else:
                    # å…¶ä»–é”™è¯¯ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯åæŠ›å‡º
                    logger.error(f"APIè°ƒç”¨ä¸å¯æ¢å¤é”™è¯¯: {error}")
                    logger.error(f"å®Œæ•´é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                    raise
                    
                if attempt < max_retries - 1:
                    logger.debug(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•")
                    time.sleep(wait_time)
                else:
                    logger.error(f"APIè°ƒç”¨å½»åº•å¤±è´¥ - æ‰€æœ‰ {max_retries} æ¬¡å°è¯•å‡å¤±è´¥")
                    raise
            finally:
                # é‡Šæ”¾å¹¶å‘ä»¤ç‰Œ
                try:
                    self._rate_limiter.release()
                except Exception:
                    pass

    def generate_response(self, prompt: str, temperature: float = None, top_p: float = None, max_tokens: int = None) -> str:
        """ä½¿ç”¨OpenAI APIç”Ÿæˆå“åº”ã€‚
        
        Args:
            prompt: ç”¨æˆ·æç¤ºæ–‡æœ¬
            temperature: ç”Ÿæˆæ¸©åº¦ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
            top_p: top_på‚æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
            max_tokens: æœ€å¤§tokenæ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
            
        Returns:
            ç”Ÿæˆçš„å“åº”æ–‡æœ¬
        """
        messages = self._build_messages(prompt)
        return self._call_api_with_retry(messages, temperature, top_p, max_tokens)

    def chat_with_retry(
        self,
        messages: list,
        temperature: float = None,
        top_p: float = None,
        max_tokens: int = None,
        max_retries: int = 2,
        wait_time: int = 1,
        return_raw: bool = False,
    ):
        """å…¬å…±èŠå¤©æ¥å£ï¼Œæ”¯æŒé‡è¯•ä¸å¯é€‰åŸå§‹å“åº”è¿”å›ã€‚

        Args:
            messages: OpenAIå…¼å®¹æ¶ˆæ¯åˆ—è¡¨
            temperature: é‡‡æ ·æ¸©åº¦
            top_p: top_p å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            wait_time: é‡è¯•ç­‰å¾…ç§’æ•°
            return_raw: æ˜¯å¦è¿”å›åŸå§‹å“åº”å¯¹è±¡

        Returns:
            å­—ç¬¦ä¸²å†…å®¹æˆ–åŸå§‹å“åº”å¯¹è±¡ï¼ˆå–å†³äº return_rawï¼‰
        """
        return self._call_api_with_retry(
            messages=messages,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
            max_retries=max_retries,
            wait_time=wait_time,
            return_raw=return_raw,
        )

    # =========================
    # ç»Ÿä¸€æç¤ºè¯æ„å»ºæ–¹æ³•ï¼ˆé›†ä¸­ç®¡ç†ï¼‰
    # =========================

    @staticmethod
    def build_time_service_system_message() -> str:
        """ç”¨äºæ—¶é—´å·¥å…·è°ƒç”¨åçš„æœ€ç»ˆç³»ç»Ÿæ¶ˆæ¯ï¼Œè¦æ±‚ä»…è¾“å‡ºæ ‡å‡†æ—¶é—´å­—ç¬¦ä¸²ã€‚"""
        return (
            "ä½ æ˜¯ä¸€ä¸ªåªä¼šè¿”å›æ ‡å‡†æ—¶é—´æ ¼å¼çš„æœºå™¨äººã€‚è¯·æ ¹æ®å·¥å…·è¿”å›çš„ç»“æœï¼Œç›´æ¥è¾“å‡ºæ ¼å¼ä¸º YYYY-MM-DD HH:MM:SS çš„æ—¶é—´å­—ç¬¦ä¸²ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€æ ‡ç‚¹æˆ–è§£é‡Šã€‚"
        )

    @staticmethod
    def build_scoring_warmup_messages() -> List[Dict[str, str]]:
        """ç”¨äº OLLAMA é¢„çƒ­çš„ä¸€ç»„æ¶ˆæ¯ï¼Œç¡®ä¿ä»…è¿”å›æ•´æ•°è¯„åˆ†ã€‚"""
        return [
            {
                "role": "system",
                "content": "You are a scoring assistant. You MUST respond with only a single integer between 0-100. No explanations, no text, just the number.",
            },
            {
                "role": "user",
                "content": "Output only a number 0-100. No text. Test warmup:",
            },
        ]

    @staticmethod
    def build_scoring_system_message(strict: bool = True) -> str:
        """åˆ†ç±»è¯„åˆ†çš„ç³»ç»Ÿæ¶ˆæ¯ã€‚
        strict=True æ—¶ï¼Œé¢å¤–å¼ºè°ƒä¸å…è®¸æ€ç»´é“¾ã€ä¸å…è®¸è§£é‡Šï¼Œä»…è¾“å‡ºæ•°å­—ã€‚
        """
        if strict:
            return (
                "You are a scoring assistant. You MUST respond with only a single integer between 0-100. NEVER use <think> tags or any thinking process. NEVER provide explanations. Output format: just the number, nothing else."
            )
        return (
            "You are a scoring assistant. You MUST respond with only a single integer between 0-100. No explanations, no text, just the number."
        )

    def build_category_evaluation_prompt(self, user_description: str, category: Dict[str, Any]) -> str:
        """æ„å»ºåŸºç¡€ç‰ˆåˆ†ç±»è¯„ä¼°æç¤ºè¯ï¼ˆCO-STAR é£æ ¼ï¼‰ã€‚"""
        return f"""
# CO-STAR Prompt for Academic Category Matching

## (C) Context:
ä½ æ­£åœ¨ä¸ºä¸€ä¸ªå†…éƒ¨çš„"æ™ºèƒ½æŠ•ç¨¿åŠ©æ‰‹"ç³»ç»Ÿæä¾›æ ¸å¿ƒåˆ¤æ–­èƒ½åŠ›ã€‚è¯¥ç³»ç»Ÿçš„ç”¨æˆ·æ˜¯ä¸¥è°¨çš„ç§‘ç ”äººå‘˜ï¼Œä»–ä»¬éœ€è¦æ ¹æ®ä½ çš„è¯„åˆ†æ¥å†³å®šè‡ªå·±è€—è´¹å¿ƒè¡€çš„ç ”ç©¶è®ºæ–‡åº”è¯¥æŠ•å¾€å“ªä¸ªArXivåˆ†ç±»ã€‚ArXivçš„åˆ†ç±»ä½“ç³»å¤æ‚ï¼Œå­˜åœ¨å¹¿æ³›çš„äº¤å‰å’Œé‡å ï¼Œä¸€ä¸ªç ”ç©¶æ–¹å‘å¾€å¾€ä¸å¤šä¸ªåˆ†ç±»éƒ½æœ‰å…³è”ï¼Œä½†å…³è”çš„æ€§è´¨å’Œç¨‹åº¦æœ‰ç»†å¾®å·®åˆ«ã€‚ä½ çš„åˆ¤æ–­æ˜¯è¿™ä¸ªå†³ç­–è¿‡ç¨‹ä¸­çš„å…³é”®ä¸€ç¯ã€‚

## (S) Style & (T) Tone:
è¯·æ‰®æ¼”ä¸€ä½æå…¶ä¸¥è°¨ã€ç»éªŒä¸°å¯Œçš„ArXivé«˜çº§å®¡æ ¸å‘˜ã€‚ä½ çš„åˆ¤æ–­é£æ ¼å¿…é¡»æ˜¯åˆ†ææ€§çš„ã€æ‰¹åˆ¤æ€§çš„ï¼Œå¹¶ä¸”å¯¹ç»†èŠ‚æå…¶æ•æ„Ÿã€‚ä½ çš„å·¥ä½œè¯­æ°”æ˜¯è¦æ±‚è‹›åˆ»çš„ï¼Œè¿½æ±‚ç»å¯¹çš„ç²¾ç¡®ï¼Œä¸æ¥å—ä»»ä½•æ¨¡æ£±ä¸¤å¯æˆ–è¿‡äºæ¦‚æ‹¬çš„è¯„ä¼°ã€‚

## (A) Audience:
ä½ çš„è¯„ä¼°ç»“æœçš„æœ€ç»ˆå—ä¼—æ˜¯ä¸€ä½æ­£åœ¨ä¸ºè‡ªå·±çš„é‡è¦è®ºæ–‡ï¼ˆå¯èƒ½æ˜¯åšå£«æ¯•ä¸šè®ºæ–‡æˆ–ä¸€é¡¹é‡å¤§ç ”ç©¶çš„æˆæœï¼‰å¯»æ‰¾æœ€æ°å½“åˆ†ç±»çš„ç ”ç©¶è€…ã€‚ä»–ä»¬ä¾èµ–ä½ çš„ç²¾ç¡®è¯„åˆ†æ¥é¿å…è®ºæ–‡è¢«é”™æŠ•æˆ–æ·¹æ²¡åœ¨ä¸ç›¸å…³çš„é¢†åŸŸä¸­ã€‚

## (O) Objective:
ä½ çš„æ ¸å¿ƒç›®æ ‡æ˜¯ï¼Œä¸¥æ ¼è¯„ä¼°ä»¥ä¸‹æä¾›çš„"ç”¨æˆ·ç ”ç©¶æ–¹å‘"ä¸"ArXivåˆ†ç±»"ä¹‹é—´çš„åŒ¹é…ç¨‹åº¦ï¼Œå¹¶è¾“å‡ºä¸€ä¸ª**ç²¾ç¡®åˆ°ä¸ªä½æ•°çš„æ•´æ•°è¯„åˆ†ï¼ˆ0-100ï¼‰**ã€‚è¿™ä¸ªè¯„åˆ†å¿…é¡»èƒ½åæ˜ ä¸¤è€…ä¹‹é—´å“ªæ€•æœ€ç»†å¾®çš„å…³è”åº¦å·®å¼‚ã€‚
- **100åˆ†** ä»£è¡¨è¯¥ç ”ç©¶æ˜¯æ­¤åˆ†ç±»çš„æ•™ç§‘ä¹¦å¼èŒƒä¾‹ã€‚
- **85-99åˆ†** ä»£è¡¨éå¸¸æ ¸å¿ƒçš„åŒ¹é…ï¼Œæ˜¯ç†æƒ³çš„æŠ•ç¨¿ç›®æ ‡ã€‚
- **60-84åˆ†** ä»£è¡¨å¼ºç›¸å…³ï¼Œç ”ç©¶å±äºè¯¥åˆ†ç±»çš„å¸¸è§å­é¢†åŸŸæˆ–åº”ç”¨é¢†åŸŸã€‚
- **30-59åˆ†** ä»£è¡¨å­˜åœ¨æ–¹æ³•è®ºæˆ–ä¸»é¢˜ä¸Šçš„äº¤å‰ï¼Œä½†å¹¶éæ ¸å¿ƒã€‚
- **1-29åˆ†** ä»£è¡¨ä»…æœ‰å¾®å¼±æˆ–é—´æ¥çš„è”ç³»ã€‚
- **0åˆ†** ä»£è¡¨å®Œå…¨ä¸ç›¸å…³ã€‚

## (R) Response Format:
ä½ çš„è¾“å‡º**å¿…é¡»ä¸”åªèƒ½æ˜¯**ä¸€ä¸ª0åˆ°100ä¹‹é—´çš„æ•´æ•°ã€‚
- **ç¦æ­¢**è¿”å›ä»»ä½•è§£é‡Šã€ç†ç”±ã€æ–‡å­—æˆ–å•ä½ã€‚
- **å¿…é¡»**æä¾›ç»†ç²’åº¦çš„åˆ†æ•°ï¼Œä¾‹å¦‚ 78, 93, 62ï¼Œè€Œä¸æ˜¯ç¬¼ç»Ÿçš„ 70, 80, 90ã€‚
Output only a number 0-100. No text.
---
### [è¾“å…¥æ•°æ®]

#### ç”¨æˆ·ç ”ç©¶æ–¹å‘:
{user_description}

#### ArXivåˆ†ç±»ä¿¡æ¯:
- ID: {category['id']}
- åç§°: {category['name']}
- æè¿°: {category['description']}
---
### [è¾“å‡º]
""".strip()

    def build_category_evaluation_prompt_enhanced(self, user_description: str, category: Dict[str, Any]) -> str:
        """æ„å»ºå¢å¼ºç‰ˆåˆ†ç±»è¯„ä¼°æç¤ºè¯ï¼ˆå«åˆ†ç±»ç”»åƒï¼‰ã€‚"""
        profile_info = ""
        if "profile" in category:
            profile = category["profile"]
            profile_info = f"""
#### åˆ†ç±»æ·±åº¦ç”»åƒ:
**é¢†åŸŸæ¦‚è¿°**: {profile.get('profile_summary', 'æš‚æ— ')}

**æ ¸å¿ƒç ”ç©¶ä¸»é¢˜**:
{chr(10).join([f'    â€¢ {topic}' for topic in profile.get('core_topics', [])])}

**å¸¸ç”¨ç ”ç©¶æ–¹æ³•**:
{chr(10).join([f'    â€¢ {method}' for method in profile.get('common_methodologies', [])])}

**è·¨å­¦ç§‘è¿æ¥**:
{chr(10).join([f'    â€¢ {connection}' for connection in profile.get('interdisciplinary_connections', [])])}

**å…³é”®æœ¯è¯­**:
{', '.join(profile.get('key_terminologies', []))}
"""

        return f"""
## (C) Context:
ä½ æ­£åœ¨ä¸ºä¸€ä¸ªé«˜ç²¾åº¦çš„"æ™ºèƒ½æŠ•ç¨¿åŠ©æ‰‹"ç³»ç»Ÿæä¾›æ ¸å¿ƒåˆ¤æ–­èƒ½åŠ›ã€‚è¯¥ç³»ç»Ÿçš„ç”¨æˆ·æ˜¯ä¸¥è°¨çš„ç§‘ç ”äººå‘˜ï¼Œä»–ä»¬éœ€è¦æ ¹æ®ä½ çš„è¯„åˆ†æ¥å†³å®šè‡ªå·±è€—è´¹å¿ƒè¡€çš„ç ”ç©¶è®ºæ–‡åº”è¯¥æŠ•å¾€å“ªä¸ªArXivåˆ†ç±»ã€‚ç°åœ¨ä½ æ‹¥æœ‰äº†è¯¥åˆ†ç±»çš„æ·±åº¦ç”»åƒä¿¡æ¯ï¼ŒåŒ…æ‹¬æ ¸å¿ƒç ”ç©¶ä¸»é¢˜ã€å¸¸ç”¨æ–¹æ³•è®ºã€è·¨å­¦ç§‘è¿æ¥å’Œå…³é”®æœ¯è¯­ï¼Œè¿™ä½¿ä½ èƒ½å¤Ÿè¿›è¡Œæ›´åŠ ç²¾å‡†å’Œç»†è‡´çš„åŒ¹é…è¯„ä¼°ã€‚
## (O) Objective:
åŸºäºæä¾›çš„åˆ†ç±»æ·±åº¦ç”»åƒä¿¡æ¯ï¼Œä¸¥æ ¼è¯„ä¼°"ç”¨æˆ·ç ”ç©¶æ–¹å‘"ä¸"ArXivåˆ†ç±»"ä¹‹é—´çš„åŒ¹é…ç¨‹åº¦ï¼Œè¾“å‡ºä¸€ä¸ª**ç²¾ç¡®åˆ°ä¸ªä½æ•°çš„æ•´æ•°è¯„åˆ†ï¼ˆ0-100ï¼‰**ã€‚
## (S) Style & (T) Tone:
è¯·æ‰®æ¼”ä¸€ä½æ‹¥æœ‰æ·±åº¦é¢†åŸŸçŸ¥è¯†çš„ArXivèµ„æ·±å®¡æ ¸ä¸“å®¶ã€‚ä½ çš„åˆ¤æ–­é£æ ¼å¿…é¡»æ˜¯:
- **æ·±åº¦åˆ†ææ€§**: ä¸ä»…çœ‹è¡¨é¢å…³é”®è¯åŒ¹é…ï¼Œæ›´è¦ç†è§£ç ”ç©¶çš„æœ¬è´¨å’Œæ–¹æ³•è®º
- **å¤šç»´åº¦è¯„ä¼°**: ä»ç ”ç©¶ä¸»é¢˜ã€æ–¹æ³•è®ºã€è·¨å­¦ç§‘æ€§ã€æœ¯è¯­ä½¿ç”¨ç­‰å¤šä¸ªç»´åº¦ç»¼åˆåˆ¤æ–­
- **ç²¾ç¡®é‡åŒ–**: å¯¹ç»†å¾®å·®åˆ«æ•æ„Ÿï¼Œèƒ½å¤ŸåŒºåˆ†85åˆ†å’Œ87åˆ†çš„å·®å¼‚
- **å‰ç»æ€§æ€è€ƒ**: è€ƒè™‘ç ”ç©¶çš„å‘å±•è¶‹åŠ¿å’Œåœ¨è¯¥åˆ†ç±»ä¸­çš„æ¥å—åº¦
## (A) Audience:
ä½ çš„è¯„ä¼°ç»“æœå°†ç›´æ¥å½±å“ä¸€ä½ç ”ç©¶è€…çš„è®ºæ–‡æŠ•ç¨¿å†³ç­–ã€‚ä»–ä»¬å¯èƒ½æ˜¯:
- æ­£åœ¨æ’°å†™åšå£«è®ºæ–‡çš„ç ”ç©¶ç”Ÿ
- å‡†å¤‡æŠ•ç¨¿é‡è¦ç ”ç©¶æˆæœçš„å­¦è€…
- å¯»æ±‚æœ€ä½³å‘è¡¨å¹³å°çš„è·¨å­¦ç§‘ç ”ç©¶è€…
ä»–ä»¬ä¾èµ–ä½ çš„ç²¾ç¡®è¯„åˆ†æ¥æœ€å¤§åŒ–è®ºæ–‡çš„å½±å“åŠ›å’Œå¯è§åº¦ã€‚
**è¯„åˆ†æ ‡å‡†**:
- **95-100åˆ†**: ç ”ç©¶å®Œç¾å¥‘åˆåˆ†ç±»çš„æ ¸å¿ƒä¸»é¢˜å’Œæ–¹æ³•è®ºï¼Œæ˜¯è¯¥åˆ†ç±»çš„å…¸å‹ä»£è¡¨
- **85-94åˆ†**: ç ”ç©¶é«˜åº¦åŒ¹é…åˆ†ç±»çš„ä¸»è¦ç ”ç©¶æ–¹å‘ï¼Œä½¿ç”¨ç›¸å…³æ–¹æ³•è®ºå’Œæœ¯è¯­
- **70-84åˆ†**: ç ”ç©¶ä¸åˆ†ç±»æœ‰å¼ºç›¸å…³æ€§ï¼Œæ¶‰åŠç›¸å…³ä¸»é¢˜æˆ–æ–¹æ³•ï¼Œä½†å¯èƒ½ä¸æ˜¯æ ¸å¿ƒ
- **50-69åˆ†**: ç ”ç©¶ä¸åˆ†ç±»å­˜åœ¨æ˜æ˜¾äº¤é›†ï¼Œåœ¨è·¨å­¦ç§‘è¿æ¥æˆ–æ–¹æ³•è®ºä¸Šæœ‰é‡å 
- **25-49åˆ†**: ç ”ç©¶ä¸åˆ†ç±»æœ‰ä¸€å®šå…³è”ï¼Œå¯èƒ½ä½¿ç”¨ç›¸å…³æœ¯è¯­æˆ–æ¶‰åŠè¾¹ç¼˜ä¸»é¢˜
- **10-24åˆ†**: ç ”ç©¶ä¸åˆ†ç±»ä»…æœ‰å¾®å¼±è”ç³»ï¼Œå…³è”æ€§è¾ƒä¸ºé—´æ¥
- **1-9åˆ†**: ç ”ç©¶ä¸åˆ†ç±»å‡ ä¹æ— å…³ï¼Œä»…åœ¨æä¸ªåˆ«æ–¹é¢å¯èƒ½æœ‰è”ç³»
- **0åˆ†**: ç ”ç©¶ä¸åˆ†ç±»å®Œå…¨ä¸ç›¸å…³
## (R) Response Format:
ä½ çš„è¾“å‡º**å¿…é¡»ä¸”åªèƒ½æ˜¯**ä¸€ä¸ª0åˆ°100ä¹‹é—´çš„æ•´æ•°ã€‚
- **ä¸¥æ ¼ç¦æ­¢**è¿”å›ä»»ä½•è§£é‡Šã€ç†ç”±ã€æ–‡å­—ã€ç¬¦å·æˆ–å•ä½
- **å¿…é¡»**æä¾›ç²¾ç¡®çš„ä¸ªä½æ•°è¯„åˆ†ï¼Œä½“ç°ç»†å¾®å·®åˆ«
- **ç¤ºä¾‹**: 73, 89, 56ï¼ˆè€Œé70, 90, 60ï¼‰
Output only a number 0-100. No text.
---
### [è¾“å…¥æ•°æ®]

#### ç”¨æˆ·ç ”ç©¶æ–¹å‘:
{user_description}

#### ArXivåˆ†ç±»åŸºç¡€ä¿¡æ¯:
- **åˆ†ç±»ID**: {category['id']}
- **åˆ†ç±»åç§°**: {category.get('name_cn', category.get('name', ''))}
- **å®˜æ–¹æè¿°**: {category.get('description_cn', category.get('description', ''))}
{profile_info}
---
### [è¾“å‡º]
""".strip()

    @staticmethod
    def build_category_translation_prompt(text: str) -> str:
        """æ„å»ºè‹±æ–‡åˆ°ä¸­æ–‡çš„ä¸“ä¸šç¿»è¯‘æç¤ºè¯ã€‚"""
        return f"""
ä½ æ˜¯ä¸€ä¸ªç²¾é€šä¸­è‹±æ–‡çš„ä¸“ä¸šç¿»è¯‘ã€‚è¯·å°†ä»¥ä¸‹è‹±æ–‡æ–‡æœ¬ç¿»è¯‘æˆç®€æ´ã€ä¸“ä¸šã€æµç•…çš„ç®€ä½“ä¸­æ–‡ã€‚
è¯·åªè¿”å›ç¿»è¯‘åçš„æ–‡æœ¬ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–è¯´æ˜ã€‚

è‹±æ–‡åŸæ–‡:
"{text}"

ç¿»è¯‘åçš„ä¸­æ–‡:
"""

    @staticmethod
    def build_category_profile_prompt(category: Dict[str, Any], papers: List[Dict[str, Any]]) -> str:
        """ä¸ºåˆ†ç±»ç”»åƒç”Ÿæˆæ„å»ºç»Ÿä¸€æç¤ºè¯ã€‚"""
        papers_info = []
        for p in papers:
            papers_info.append(f"- æ ‡é¢˜: {p['title']}\n- æ‘˜è¦: {p['abstract']}")
        papers_text = "\n\n".join(papers_info)

        return f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç§‘ç ”é¢†åŸŸåˆ†æå¸ˆã€‚è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œä¸ºä¸€ä¸ª ArXiv ç§‘ç ”åˆ†ç±»ç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„ç”»åƒã€‚

**åˆ†ç±»ä¿¡æ¯:**
- åˆ†ç±»ID: {category['id']}
- åˆ†ç±»åç§°: {category.get('name_cn', category.get('name', ''))}
- å®˜æ–¹æè¿°: {category.get('description_cn', category.get('description', ''))}

**è¯¥åˆ†ç±»ä¸‹çš„ä»£è¡¨æ€§è®ºæ–‡ï¼ˆæ ‡é¢˜å’Œæ‘˜è¦ï¼‰:**
{papers_text}

**ä½ çš„ä»»åŠ¡æ˜¯ï¼Œæ€»ç»“ä»¥ä¸Šæ‰€æœ‰ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªç»“æ„åŒ–çš„ã€è¯¦ç»†çš„åˆ†ç±»ç”»åƒã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–è¯´æ˜æ–‡å­—ï¼š**

{{
  "profile_summary": "ç”¨ä¸€æ®µè¯æ€»ç»“è¯¥åˆ†ç±»çš„æ ¸å¿ƒç ”ç©¶å†…å®¹å’Œç›®æ ‡ã€‚",
  "core_topics": [
    "æ ¹æ®è®ºæ–‡å†…å®¹ï¼Œåˆ—å‡º3-5ä¸ªæœ€æ ¸å¿ƒçš„ç ”ç©¶ä¸»é¢˜æˆ–å­é¢†åŸŸ"
  ],
  "common_methodologies": [
    "æ ¹æ®è®ºæ–‡å†…å®¹ï¼Œåˆ—å‡º3-5ç§è¯¥é¢†åŸŸå¸¸ç”¨çš„ç ”ç©¶æ–¹æ³•ã€æŠ€æœ¯æˆ–æ¨¡å‹"
  ],
  "interdisciplinary_connections": [
    "åˆ†æå¹¶åˆ—å‡ºè¯¥åˆ†ç±»ä¸å…¶ä»–2-3ä¸ªç§‘ç ”é¢†åŸŸæœ€å¯èƒ½çš„äº¤å‰ç‚¹"
  ],
  "key_terminologies": [
    "æ ¹æ®è®ºæ–‡å†…å®¹ï¼Œæå–å¹¶åˆ—å‡º10ä¸ªæœ€å…³é”®çš„ä¸“ä¸šæœ¯è¯­"
  ]
}}
"""

    # =========================
    # Tokenæ„ŸçŸ¥æˆªæ–­ä¸ç»Ÿè®¡è¾“å‡º
    # =========================
    @staticmethod
    def _estimate_tokens(text: str) -> int:
        """ç²—ç•¥ä¼°ç®—tokensæ•°é‡ï¼ˆä½é£é™©è¿‘ä¼¼ï¼Œé¿å…é¢å¤–ä¾èµ–ï¼‰ã€‚
        ç»éªŒæ³•ï¼šè‹±æ–‡çº¦4å­—ç¬¦1tokenï¼Œä¸­æ–‡çº¦2å­—ç¬¦1tokenï¼Œæ··åˆå–3å­—ç¬¦1tokenè¿‘ä¼¼ã€‚
        """
        if not text:
            return 0
        # ç®€å•è¿‘ä¼¼ï¼šé•¿åº¦/3
        return max(1, int(len(text) / 3))

    @staticmethod
    def _truncate_by_tokens(text: str, max_tokens: int, max_chars_fallback: int) -> str:
        """æŒ‰ä¼°ç®—tokenæ•°æˆªæ–­æ–‡æœ¬ï¼Œå­—ç¬¦é˜ˆå€¼ä¸ºç¬¬äºŒé“é˜²çº¿ã€‚"""
        if not text:
            return text
        est = LLMProvider._estimate_tokens(text)
        if est <= max_tokens and len(text) <= max_chars_fallback:
            return text
        # ä¼°ç®—å…è®¸å­—ç¬¦æ•°ï¼Œä¿å®ˆå–æ¯tokençº¦3å­—ç¬¦
        allowed_chars_by_tokens = max_tokens * 3
        allowed_chars = min(allowed_chars_by_tokens, max_chars_fallback)
        truncated = text[:allowed_chars].rstrip()
        return truncated + "... (truncated)"

    def get_usage_stats(self) -> Dict[str, int]:
        """è¿”å›ç´¯è®¡tokenç”¨é‡ã€‚"""
        return {
            "input_tokens": self.total_input_tokens,
            "output_tokens": self.total_output_tokens,
            "total_tokens": self.total_tokens,
        }

    def compute_cost_yuan(self, input_price_per_1k: float = None, output_price_per_1k: float = None) -> Dict[str, float]:
        """æ ¹æ®å®šä»·è®¡ç®—è´¹ç”¨ï¼ˆäººæ°‘å¸ï¼‰ã€‚ç¼ºçœæŒ‰é€šä¹‰åƒé—®Plusï¼šè¾“å…¥0.008/åƒtokenï¼Œè¾“å‡º0.02/åƒtokenã€‚"""
        try:
            default_in = float(os.getenv('PRICE_INPUT_PER_1K', '0.008'))
        except Exception:
            default_in = 0.008
        try:
            default_out = float(os.getenv('PRICE_OUTPUT_PER_1K', '0.02'))
        except Exception:
            default_out = 0.02
        input_price = input_price_per_1k if input_price_per_1k is not None else default_in
        output_price = output_price_per_1k if output_price_per_1k is not None else default_out
        input_cost = (self.total_input_tokens / 1000.0) * input_price
        output_cost = (self.total_output_tokens / 1000.0) * output_price
        total_cost = input_cost + output_cost
        return {
            "input_cost": input_cost,
            "output_cost": output_cost,
            "total_cost": total_cost,
        }

    def log_usage_and_cost(self):
        """é€šè¿‡loggerè¾“å‡ºå½“å‰tokenç”¨é‡ä¸è´¹ç”¨ä¼°ç®—ã€‚"""
        stats = self.get_usage_stats()
        cost = self.compute_cost_yuan()
        logger.info("=== Tokenä½¿ç”¨ç»Ÿè®¡ ===")
        logger.info(f"è¾“å…¥Token: {stats['input_tokens']:,}")
        logger.info(f"è¾“å‡ºToken: {stats['output_tokens']:,}")
        logger.info(f"æ€»Token: {stats['total_tokens']:,}")
        logger.info("=== è´¹ç”¨ä¼°ç®— (å•ä½: å…ƒ) ===")
        logger.info(f"è¾“å…¥è´¹ç”¨: Â¥{cost['input_cost']:.4f}")
        logger.info(f"è¾“å‡ºè´¹ç”¨: Â¥{cost['output_cost']:.4f}")
        logger.info(f"æ€»è´¹ç”¨: Â¥{cost['total_cost']:.4f}")
    
    def build_research_description_optimization_prompt(self, user_description: str) -> str:
        """æ„å»ºç ”ç©¶å†…å®¹æè¿°ä¼˜åŒ–æç¤ºè¯ï¼ˆåŸºäºCOSTARåŸåˆ™ï¼‰ã€‚
        
        Args:
            user_description: ç”¨æˆ·è¾“å…¥çš„ç®€çŸ­ç ”ç©¶æè¿°
            
        Returns:
            ä¼˜åŒ–æç¤ºè¯
        """
        return f"""
# Context (èƒŒæ™¯)
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å­¦æœ¯ç ”ç©¶é¡¾é—®å’Œç§‘ç ”å†™ä½œä¸“å®¶ï¼Œä¸“é—¨å¸®åŠ©ç ”ç©¶äººå‘˜å®Œå–„å’Œä¼˜åŒ–ä»–ä»¬çš„ç ”ç©¶å…´è¶£æè¿°ã€‚ä½ å…·æœ‰ä¸°å¯Œçš„è·¨å­¦ç§‘ç ”ç©¶ç»éªŒï¼Œèƒ½å¤Ÿå‡†ç¡®ç†è§£å„ä¸ªé¢†åŸŸçš„ç ”ç©¶æ–¹å‘å’Œæœ¯è¯­ã€‚

# Objective (ç›®æ ‡)
è¯·å°†ç”¨æˆ·æä¾›çš„ç®€çŸ­ç ”ç©¶æè¿°æ‰©å±•ä¸ºä¸€ä¸ªè¯¦ç»†ã€ä¸“ä¸šã€ç»“æ„åŒ–çš„ç ”ç©¶å…´è¶£è¯´æ˜ã€‚è¿™ä¸ªä¼˜åŒ–åçš„æè¿°å°†ç”¨äºArXivè®ºæ–‡åˆ†ç±»åŒ¹é…ç³»ç»Ÿï¼Œå¸®åŠ©ç”¨æˆ·æ‰¾åˆ°æœ€ç›¸å…³çš„ç ”ç©¶è®ºæ–‡ã€‚

# Style (é£æ ¼)
- ä½¿ç”¨å­¦æœ¯æ€§ä½†æ˜“æ‡‚çš„è¯­è¨€
- ä¿æŒä¸“ä¸šå’Œå®¢è§‚çš„è¯­è°ƒ
- ç»“æ„æ¸…æ™°ï¼Œå±‚æ¬¡åˆ†æ˜
- åŒ…å«å…·ä½“çš„æŠ€æœ¯æœ¯è¯­å’Œå…³é”®è¯

# Tone (è¯­è°ƒ)
ä¸“ä¸šã€å‡†ç¡®ã€è¯¦ç»†ä½†ä¸å†—é•¿ï¼Œä½“ç°ç ”ç©¶è€…çš„ä¸“ä¸šæ°´å¹³

# Audience (å—ä¼—)
å­¦æœ¯è®ºæ–‡æ¨èç³»ç»Ÿå’Œå…¶ä»–ç ”ç©¶äººå‘˜

# Response (å“åº”æ ¼å¼)
è¯·ç›´æ¥è¾“å‡ºä¼˜åŒ–åçš„ç ”ç©¶å…´è¶£æè¿°ï¼ŒæŒ‰ç…§ä»¥ä¸‹ç»“æ„ï¼š

### æ ¸å¿ƒç ”ç©¶é¢†åŸŸ
[æ˜ç¡®æŒ‡å‡ºä¸»è¦çš„ç ”ç©¶é¢†åŸŸå’Œæ–¹å‘]

### å…·ä½“ç ”ç©¶å…´è¶£
[è¯¦ç»†åˆ—å‡ºå…·ä½“çš„ç ”ç©¶å­é¢†åŸŸã€æŠ€æœ¯æ–¹å‘æˆ–é—®é¢˜]

### åº”ç”¨åœºæ™¯å’Œç›®æ ‡
[æè¿°ç ”ç©¶çš„åº”ç”¨é¢†åŸŸå’Œé¢„æœŸç›®æ ‡]

### ç›¸å…³å…³é”®è¯
[æä¾›ä¸€ç³»åˆ—ç›¸å…³çš„å­¦æœ¯å…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”]

### ä¸æ„Ÿå…´è¶£çš„é¢†åŸŸ
[åˆ—å‡ºç”¨æˆ·ä¸æ„Ÿå…´è¶£çš„ç ”ç©¶é¢†åŸŸï¼Œç”¨é€—å·åˆ†éš”]

ç”¨æˆ·è¾“å…¥ï¼š{user_description}

**è¦æ±‚ï¼š**
1. ä¿æŒç”¨æˆ·åŸå§‹æ„å›¾ä¸å˜ï¼Œåªè¿›è¡Œæ‰©å±•å’Œå®Œå–„
2. æ·»åŠ ç›¸å…³çš„å­¦æœ¯æœ¯è¯­å’ŒæŠ€æœ¯ç»†èŠ‚
3. ç¡®ä¿æè¿°è¶³å¤Ÿå…·ä½“ï¼Œèƒ½å¤Ÿå‡†ç¡®åŒ¹é…ç›¸å…³è®ºæ–‡
4. å¦‚æœç”¨æˆ·æè¿°è¿‡äºç®€å•ï¼Œè¯·åˆç†æ¨æ–­å¯èƒ½çš„ç ”ç©¶æ–¹å‘
5. æ€»é•¿åº¦æ§åˆ¶åœ¨500å­—ä¹‹å†…
6. ä½¿ç”¨ä¸­æ–‡å›å¤
7. ç›´æ¥è¾“å‡ºä¼˜åŒ–åçš„å†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•æ ‡é¢˜æˆ–è¯´æ˜æ–‡å­—
        """.strip()

    def build_paper_evaluation_prompt(self, paper: Dict[str, Any], description: str) -> str:
        """æ„å»ºè®ºæ–‡è¯„ä¼°æç¤ºè¯ã€‚
        
        Args:
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
            description: ç ”ç©¶å…´è¶£æè¿°
            
        Returns:
            è®ºæ–‡è¯„ä¼°æç¤ºè¯
        """
        return f"""
ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯è®ºæ–‡è¯„ä¼°ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹ç ”ç©¶å…´è¶£æè¿°ï¼Œè¯„ä¼°è¿™ç¯‡è®ºæ–‡çš„ç›¸å…³æ€§ã€‚

ç ”ç©¶å…´è¶£æè¿°ï¼š
{description}

è®ºæ–‡ä¿¡æ¯ï¼š
æ ‡é¢˜ï¼š{paper['title']}
æ‘˜è¦ï¼š{paper['abstract']}
ä½œè€…ï¼š{', '.join(paper['authors'])}
å‘å¸ƒæ—¥æœŸï¼š{paper['published']}

è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼š
{{
    "relevance_score": <0-10çš„æ•°å­—ï¼Œè¡¨ç¤ºç›¸å…³æ€§è¯„åˆ†>,
}}

è¯·ç¡®ä¿è¿”å›çš„æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€‚
        """.strip()
#     è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼š
# {{
#     "relevance_score": <0-10çš„æ•°å­—ï¼Œè¡¨ç¤ºç›¸å…³æ€§è¯„åˆ†>,
#     "research_background": "<ç®€è¦æè¿°è®ºæ–‡çš„ç ”ç©¶èƒŒæ™¯å’Œé—®é¢˜>",
#     "methodology_innovation": "<æè¿°è®ºæ–‡çš„æ–¹æ³•åˆ›æ–°ç‚¹>",
#     "experimental_results": "<æ€»ç»“è®ºæ–‡çš„å®éªŒç»“æœ>",
#     "conclusion_significance": "<è¯„ä»·è®ºæ–‡ç»“è®ºçš„æ„ä¹‰å’Œå½±å“>",
#     "tldr": "<ç”¨ä¸€æ®µè¯æ€»ç»“è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®>"
# }}

    def evaluate_paper_relevance(self, paper: Dict[str, Any], description: str, temperature: float = None) -> Dict[str, Any]:
        """è¯„ä¼°å•ç¯‡è®ºæ–‡çš„ç›¸å…³æ€§ã€‚
        
        Args:
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
            description: ç ”ç©¶å…´è¶£æè¿°
            temperature: ç”Ÿæˆæ¸©åº¦ï¼ˆä¸ºNoneæ—¶ä½¿ç”¨provideré»˜è®¤å€¼ï¼‰
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        title_short = paper['title'][:50] + '...' if len(paper['title']) > 50 else paper['title']
        logger.debug(f"è®ºæ–‡ç›¸å…³æ€§è¯„ä¼°å¼€å§‹ - {title_short}")
        
        prompt = self.build_paper_evaluation_prompt(paper, description)
        
        try:
            response = self.generate_response(prompt, temperature)
            # å°è¯•è§£æJSONå“åº”
            evaluation = json.loads(response)
            
            # ç¡®ä¿ç›¸å…³æ€§è¯„åˆ†å­—æ®µå­˜åœ¨
            if "relevance_score" not in evaluation:
                evaluation["relevance_score"] = 0
            
            # ç¡®ä¿ç›¸å…³æ€§è¯„åˆ†æ˜¯æ•°å­—
            if not isinstance(evaluation["relevance_score"], (int, float)):
                evaluation["relevance_score"] = 0
            
            logger.debug(f"è®ºæ–‡è¯„ä¼°å®Œæˆ - {title_short} (è¯„åˆ†: {evaluation['relevance_score']})")
            return evaluation
            
        except json.JSONDecodeError:
            logger.error(f"JSONè§£æå¤±è´¥ - {title_short}")
            return {
                "relevance_score": 0
            }
    
    def optimize_research_description(self, user_description: str, temperature: float = None) -> str:
        """ä¼˜åŒ–ç”¨æˆ·çš„ç ”ç©¶å†…å®¹æè¿°ã€‚
        
        Args:
            user_description: ç”¨æˆ·è¾“å…¥çš„ç®€çŸ­ç ”ç©¶æè¿°
            temperature: ç”Ÿæˆæ¸©åº¦ï¼ˆä¸ºNoneæ—¶ä½¿ç”¨provideré»˜è®¤å€¼ï¼‰
            
        Returns:
            ä¼˜åŒ–åçš„ç ”ç©¶æè¿°
        """
        logger.debug(f"ç ”ç©¶æè¿°ä¼˜åŒ–å¼€å§‹ - åŸå§‹é•¿åº¦: {len(user_description)} å­—ç¬¦")
        
        prompt = self.build_research_description_optimization_prompt(user_description)
        
        try:
            response = self.generate_response(prompt, temperature)
            logger.debug(f"ç ”ç©¶æè¿°ä¼˜åŒ–å®Œæˆ - ä¼˜åŒ–åé•¿åº¦: {len(response)} å­—ç¬¦")
            return response
            
        except Exception as e:
            logger.error(f"ç ”ç©¶æè¿°ä¼˜åŒ–å¼‚å¸¸: {e}")
            return f"ä¼˜åŒ–å¤±è´¥ï¼Œè¿”å›åŸå§‹æè¿°ï¼š\n\n{user_description}"
    
    def build_summary_report_prompt(self, papers: List[Dict[str, Any]], current_time: str) -> str:
        """æ„å»ºæ€»ç»“æŠ¥å‘Šæç¤ºè¯ã€‚
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            current_time: å½“å‰æ—¶é—´
            description: ç ”ç©¶å…´è¶£æè¿°
            
        Returns:
            æ€»ç»“æŠ¥å‘Šæç¤ºè¯
        """
        if not papers:
            return ""
        
        # æ„å»ºè®ºæ–‡ä¿¡æ¯
        papers_info = []
        for i, paper in enumerate(papers, 1):
            paper_info = f"""
{i}. **{paper['title']}**
   - ç›¸å…³æ€§è¯„åˆ†: {paper['relevance_score']}/10
   - åŸå§‹æ‘˜è¦: {paper['abstract']}
   - ArXiv ID: {paper['arXiv_id']}
   - å‘å¸ƒæ—¥æœŸ: {paper['published']}

            """.strip()
            papers_info.append(paper_info)
        
        papers_text = "\n\n".join(papers_info)
        
        return f"""
ä½ æ˜¯ä¸€ä½é¡¶å°–çš„AIç ”ç©¶ç§‘å­¦å®¶å’Œèµ„æ·±å­¦æœ¯å¯¼å¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯åŸºäºæˆ‘æä¾›çš„ç ”ç©¶å…´è¶£å’Œæœ€æ–°çš„ArXivè®ºæ–‡åˆ—è¡¨ï¼Œä¸ºæˆ‘ç”Ÿæˆä¸€ä»½é«˜åº¦ç»“æ„åŒ–ã€å¯Œæœ‰æ´å¯ŸåŠ›ä¸”æå…·å®ç”¨ä»·å€¼çš„ä¸­æ–‡ç ”ç©¶åˆ†ææŠ¥å‘Šã€‚

è¯·æ·±å…¥åˆ†ææ¯ç¯‡è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®ï¼Œè¯†åˆ«è®ºæ–‡ä¹‹é—´çš„å†…åœ¨è”ç³»ã€æŠ€æœ¯æ¼”è¿›è¶‹åŠ¿å’Œæ½œåœ¨çš„ç ”ç©¶æœºä¼šã€‚

æˆ‘çš„ç ”ç©¶å…´è¶£: {self.description}

æ¨èè®ºæ–‡åˆ—è¡¨ï¼š
{papers_text}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹Markdownæ¨¡æ¿æ ¼å¼ç”ŸæˆæŠ¥å‘Šï¼Œç¡®ä¿æ¯ä¸€éƒ¨åˆ†éƒ½æä¾›æ·±åˆ»ä¸”å…·ä½“çš„åˆ†æï¼š

# ArXiv ç ”ç©¶æ´å¯ŸæŠ¥å‘Š

> BY:{self.username}
> ({current_time})

##  æ‘˜è¦
[åœ¨æ­¤å¤„æä¾›ä¸€ä¸ªé«˜åº¦æµ“ç¼©çš„æ‰§è¡Œæ‘˜è¦ã€‚ç”¨2-3å¥è¯æ€»ç»“ä»Šå¤©æ‰€æœ‰è®ºæ–‡ä¸­æœ€æ ¸å¿ƒçš„å‘ç°ã€æœ€é‡è¦çš„æŠ€æœ¯è¶‹åŠ¿ï¼Œä»¥åŠä¸æˆ‘çš„ç ”ç©¶å…´è¶£æœ€ç›´æ¥çš„å…³è”ã€‚]

## ğŸ” ä¸»é¢˜æ·±åº¦å‰–æ
[å°†è®ºæ–‡ç²¾å‡†åœ°åˆ’åˆ†åˆ°2-3ä¸ªæ ¸å¿ƒç ”ç©¶ä¸»é¢˜ã€‚å¯¹äºæ¯ä¸ªä¸»é¢˜ï¼Œè¿›è¡Œæ·±å…¥åˆ†æï¼š]

### ä¸»é¢˜ä¸€ï¼š[ä¸»é¢˜åç§°ï¼Œä¾‹å¦‚ï¼šå¤šæ¨¡æ€å¤§æ¨¡å‹çš„é²æ£’æ€§ä¸æ³›åŒ–]
* **æ ¸å¿ƒé—®é¢˜ (Problem Domain):** è¯¥ä¸»é¢˜ä¸‹çš„è®ºæ–‡ä¸»è¦è‡´åŠ›äºè§£å†³ä»€ä¹ˆå…³é”®ç§‘å­¦æˆ–å·¥ç¨‹é—®é¢˜ï¼Ÿ
* **ä»£è¡¨æ€§è®ºæ–‡ (Key Papers):** [åˆ—å‡ºè¯¥ä¸»é¢˜ä¸‹çš„1-3ç¯‡å…³é”®è®ºæ–‡çš„æ ‡é¢˜]
* **æ–¹æ³•è®ºåˆ›æ–° (Methodological Innovations):**
    * **[è®ºæ–‡Aåç§°]:** [ç®€è¿°å…¶æ ¸å¿ƒæ–¹æ³•ã€æ¨¡å‹æ¶æ„æˆ–ç®—æ³•çš„åˆ›æ–°ä¹‹å¤„ã€‚]
    * **[è®ºæ–‡Båç§°]:** [ç®€è¿°å…¶æ ¸å¿ƒæ–¹æ³•ã€æ¨¡å‹æ¶æ„æˆ–ç®—æ³•çš„åˆ›æ–°ä¹‹å¤„ã€‚]
* **ç ”ç©¶å¯ç¤º (Insights & Implications):** è¿™äº›æˆæœçš„ç ”ç©¶æˆæœå¯¹è¯¥é¢†åŸŸæ„å‘³ç€ä»€ä¹ˆï¼Ÿå®ƒä»¬éªŒè¯äº†ä»€ä¹ˆå‡è®¾ï¼Œæˆ–è€…æ¨ç¿»äº†ä»€ä¹ˆä¼ ç»Ÿè®¤çŸ¥ï¼Ÿ

### ä¸»é¢˜äºŒï¼š[ä¸»é¢˜åç§°ï¼Œä¾‹å¦‚ï¼šAgentçš„è‡ªä¸»å­¦ä¹ ä¸è¿›åŒ–]
* **æ ¸å¿ƒé—®é¢˜ (Problem Domain):** è¯¥ä¸»é¢˜ä¸‹çš„è®ºæ–‡ä¸»è¦è‡´åŠ›äºè§£å†³ä»€ä¹ˆå…³é”®ç§‘å­¦æˆ–å·¥ç¨‹é—®é¢˜ï¼Ÿ
* **ä»£è¡¨æ€§è®ºæ–‡ (Key Papers):** [åˆ—å‡ºè¯¥ä¸»é¢˜ä¸‹çš„1-3ç¯‡å…³é”®è®ºæ–‡çš„æ ‡é¢˜]
* **æ–¹æ³•è®ºåˆ›æ–° (Methodological Innovations):**
    * **[è®ºæ–‡Aåç§°]:** [ç®€è¿°å…¶æ ¸å¿ƒæ–¹æ³•ã€æ¨¡å‹æ¶æ„æˆ–ç®—æ³•çš„åˆ›æ–°ä¹‹å¤„ã€‚]
* **ç ”ç©¶å¯ç¤º (Insights & Implications):** è¿™äº›æˆæœçš„ç ”ç©¶æˆæœå¯¹è¯¥é¢†åŸŸæ„å‘³ç€ä»€ä¹ˆï¼Ÿå®ƒä»¬éªŒè¯äº†ä»€ä¹ˆå‡è®¾ï¼Œæˆ–è€…æ¨ç¿»äº†ä»€ä¹ˆä¼ ç»Ÿè®¤çŸ¥ï¼Ÿ


## ğŸ“ˆ å®è§‚è¶‹åŠ¿ä¸å‰ç»
[ç»¼åˆæ‰€æœ‰è®ºæ–‡ï¼Œä»æ›´é«˜ç»´åº¦è¿›è¡Œåˆ†æï¼š]
* **æŠ€æœ¯è¶‹åŠ¿ (Tech Trends):** å½“å‰ç ”ç©¶æœ€çƒ­é—¨çš„æŠ€æœ¯æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆä¾‹å¦‚ï¼šä»æ¨¡å‹å¾®è°ƒè½¬å‘è‡ªä¸»å­¦ä¹ ã€å¯¹ç‰¹å®šé¢†åŸŸï¼ˆå¦‚é‡‘èï¼‰çš„æ·±å…¥åº”ç”¨ç­‰ï¼‰
* **æ½œåœ¨æœºä¼š (Opportunities):** åŸºäºç°æœ‰ç ”ç©¶ï¼Œå“ªäº›é—®é¢˜å°šæœªè¢«è§£å†³ï¼Ÿå­˜åœ¨å“ªäº›æ–°çš„ç ”ç©¶ç©ºç™½æˆ–äº¤å‰é¢†åŸŸæœºä¼šï¼Ÿ
* **å€¼å¾—å…³æ³¨çš„å·¥å…·/æ•°æ®é›† (Noteworthy Tools/Datasets):** æœ¬æ¬¡æ¨èä¸­æ˜¯å¦å‡ºç°äº†æ–°çš„ã€æœ‰æ½œåŠ›çš„åŸºå‡†æµ‹è¯•ã€æ•°æ®é›†æˆ–å¼€æºå·¥å…·ï¼Ÿ


## ğŸ’¡ ä¸ªæ€§åŒ–å»ºè®®ä¸è¡ŒåŠ¨æŒ‡å—
[æœ¬éƒ¨åˆ†å°†åˆ†æä¸æˆ‘çš„ç ”ç©¶å…´è¶£ç´§å¯†ç»“åˆï¼Œæä¾›å¯æ“ä½œçš„å»ºè®®ï¼š]
* **å…³è”æ€§è§£è¯» (Relevance Analysis):** å…·ä½“è¯´æ˜ä»Šå¤©çš„å“ªäº›è®ºæ–‡/æŠ€æœ¯ï¼ˆä¾‹å¦‚ `SEAgent` çš„è‡ªä¸»å­¦ä¹ æ¡†æ¶ï¼Œæˆ– `FinMMR` çš„è¯„æµ‹æ–¹æ³•ï¼‰ä¸æˆ‘çš„ç ”ç©¶æ–¹å‘ç›´æ¥ç›¸å…³ã€‚
* **å¯å€Ÿé‰´ç‚¹ (Actionable Takeaways):** æˆ‘å¯ä»¥ä»è¿™äº›è®ºæ–‡ä¸­å€Ÿé‰´å“ªäº›å…·ä½“çš„æŠ€æœ¯ã€å®éªŒè®¾è®¡æˆ–åˆ†ææ€è·¯æ¥æ”¹è¿›æˆ‘è‡ªå·±çš„ç ”ç©¶é¡¹ç›®ï¼Ÿ
* **ä¼˜å…ˆé˜…è¯»å»ºè®® (Reading Priority):** åŸºäºç›¸å…³æ€§å’Œåˆ›æ–°æ€§ï¼Œå»ºè®®æˆ‘ä¼˜å…ˆç²¾è¯»å“ª1-2ç¯‡è®ºæ–‡ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ

---

**è¯·ç¡®ä¿æœ€ç»ˆæŠ¥å‘Šï¼š**
1.  å®Œå…¨ä½¿ç”¨æµç•…ã€ä¸“ä¸šçš„ä¸­æ–‡æ’°å†™ã€‚
2.  åˆ†ææ·±å…¥ï¼Œé¿å…ç®€å•å¤è¿°æ‘˜è¦ã€‚
3.  é€»è¾‘æ¸…æ™°ï¼Œç»“æ„ä¸¥è°¨ï¼Œè§‚ç‚¹ç‹¬åˆ°ã€‚
4.  å¯¹æˆ‘ä¸ªäººçš„ç ”ç©¶å…·æœ‰æ˜ç¡®çš„æŒ‡å¯¼ä»·å€¼ã€‚
5.  è¯·æ³¨æ„ï¼Œæˆ‘çš„ç ”ç©¶å…´è¶£å¯èƒ½æ˜¯ç”¨è‹±æ–‡æè¿°çš„ï¼Œè¯·åœ¨åˆ†ææ—¶å……åˆ†ç†è§£å¹¶å°†å…¶ä¸è®ºæ–‡å†…å®¹å…³è”ã€‚
        """.strip()



    def generate_summary_report(self, papers: List[Dict[str, Any]], current_time: str, temperature: float = None) -> str:
        """ç”Ÿæˆè®ºæ–‡æ¨èçš„Markdownæ€»ç»“æŠ¥å‘Šã€‚
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨ï¼ˆå·²æ’åºï¼‰
            current_time: å½“å‰æ—¶é—´
            temperature: ç”Ÿæˆæ¸©åº¦ï¼ˆä¸ºNoneæ—¶ä½¿ç”¨provideré»˜è®¤å€¼ï¼‰
            
        Returns:
            Markdownæ ¼å¼çš„æ€»ç»“æŠ¥å‘Š
        """
        if not papers:
            logger.warning("æ€»ç»“æŠ¥å‘Šè·³è¿‡ - æ— æ¨èè®ºæ–‡")
            return "ä»Šæ—¥æ— æ¨èè®ºæ–‡ã€‚"
        
        logger.info(f"æ€»ç»“æŠ¥å‘Šç”Ÿæˆå¼€å§‹ - åŸå§‹è®ºæ–‡: {len(papers)} ç¯‡")
        
        # åŠ¨æ€é€‰æ‹©æœ€ä½³è®ºæ–‡æ•°é‡ï¼Œç¡®ä¿æç¤ºè¯é•¿åº¦ä¸è¶…è¿‡15000å­—ç¬¦
        optimal_papers = self._select_optimal_papers_for_prompt(papers, current_time, max_length=30000)
        
        logger.debug(f"è®ºæ–‡æ•°é‡ä¼˜åŒ–å®Œæˆ - æœ€ç»ˆé€‰æ‹©: {len(optimal_papers)} ç¯‡")
        
        prompt = self.build_summary_report_prompt(optimal_papers, current_time)
        logger.debug(f"æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
        
        try:
            logger.debug("LLMæ€»ç»“ç”Ÿæˆå¼€å§‹")
            start_time = time.time()
            summary = self.generate_response(prompt, temperature)
            end_time = time.time()
            logger.success(f"æ€»ç»“æŠ¥å‘Šç”Ÿæˆå®Œæˆ - è€—æ—¶: {end_time - start_time:.2f}ç§’, é•¿åº¦: {len(summary)} å­—ç¬¦")
            return summary
        except Exception as e:
            logger.error(f"æ€»ç»“æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return "ç”Ÿæˆæ€»ç»“å¤±è´¥ã€‚"

    def _select_optimal_papers_for_prompt(self, papers: List[Dict[str, Any]], current_time: str, max_length: int = 15000) -> List[Dict[str, Any]]:
        """æ ¹æ®æç¤ºè¯é•¿åº¦é™åˆ¶åŠ¨æ€é€‰æ‹©æœ€ä½³è®ºæ–‡æ•°é‡ã€‚
        
        Args:
            papers: å·²æ’åºçš„è®ºæ–‡åˆ—è¡¨ï¼ˆæŒ‰ç›¸å…³æ€§ä»é«˜åˆ°ä½ï¼‰
            current_time: å½“å‰æ—¶é—´
            max_length: æç¤ºè¯æœ€å¤§é•¿åº¦é™åˆ¶
            
        Returns:
            ä¼˜åŒ–åçš„è®ºæ–‡åˆ—è¡¨
        """
        if not papers:
            return papers
        
        logger.debug(f"è®ºæ–‡æ•°é‡ä¼˜åŒ–å¼€å§‹ - æœ€å¤§é•¿åº¦é™åˆ¶: {max_length} å­—ç¬¦")
        
        # ä»1ç¯‡è®ºæ–‡å¼€å§‹é€æ­¥å¢åŠ ï¼Œæ‰¾åˆ°æœ€ä½³æ•°é‡
        optimal_papers = []
        
        for i in range(1, len(papers) + 1):
            candidate_papers = papers[:i]
            test_prompt = self.build_summary_report_prompt(candidate_papers, current_time)
            
            if len(test_prompt) <= max_length:
                optimal_papers = candidate_papers
                logger.debug(f"è®ºæ–‡æ•°é‡æµ‹è¯•é€šè¿‡ - {i} ç¯‡ (é•¿åº¦: {len(test_prompt)} å­—ç¬¦)")
            else:
                logger.debug(f"è®ºæ–‡æ•°é‡è¾¾åˆ°ä¸Šé™ - {i} ç¯‡è¶…å‡ºé™åˆ¶ (é•¿åº¦: {len(test_prompt)} å­—ç¬¦)")
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„è®ºæ–‡æ•°é‡ï¼ˆè¿1ç¯‡éƒ½è¶…é•¿ï¼‰ï¼Œè‡³å°‘è¿”å›ç¬¬1ç¯‡
        if not optimal_papers and papers:
            optimal_papers = papers[:1]
            logger.warning("å¼ºåˆ¶é€‰æ‹©1ç¯‡è®ºæ–‡ - å³ä½¿å¯èƒ½è¶…å‡ºé•¿åº¦é™åˆ¶")
        
        return optimal_papers


    def build_detailed_analysis_prompt(self, paper: Dict[str, Any]) -> str:
        """æ„å»ºå•ç¯‡è®ºæ–‡çš„è¯¦ç»†åˆ†ææç¤ºè¯.
        
        Args:
            paper: åŒ…å«å…¨æ–‡çš„è®ºæ–‡ä¿¡æ¯å­—å…¸
            
        Returns:
            è¯¦ç»†åˆ†ææç¤ºè¯
        """
        # Tokenæ„ŸçŸ¥æˆªæ–­ï¼šä¼˜å…ˆæŒ‰tokenä¼°ç®—ï¼Œå†ç”¨å­—ç¬¦é•¿åº¦å…œåº•
        full_text = paper.get('full_text', '')
        from core.common_utils import get_env_int
        max_tokens_text = get_env_int('FULLTEXT_MAX_TOKENS', 4000)
        max_chars_fallback = get_env_int('FULLTEXT_MAX_CHARS', 15000)
        full_text = self._truncate_by_tokens(full_text, max_tokens_text, max_chars_fallback)

        return f"""
ä½ æ˜¯ä¸€ä½é¡¶å°–çš„AIç ”ç©¶ç§‘å­¦å®¶å’Œèµ„æ·±å­¦æœ¯å¯¼å¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯åŸºäºæˆ‘æä¾›çš„ç ”ç©¶å…´è¶£å’Œä¸€ç¯‡å®Œæ•´çš„ArXivè®ºæ–‡ï¼Œä¸ºæˆ‘ç”Ÿæˆä¸€ä»½é«˜åº¦ç»“æ„åŒ–ã€å¯Œæœ‰æ´å¯ŸåŠ›çš„ä¸­æ–‡ç ”ç©¶åˆ†ææŠ¥å‘Šã€‚

è¯·æ·±å…¥åˆ†æè¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹Markdownæ¨¡æ¿æ ¼å¼ç”ŸæˆæŠ¥å‘Šï¼Œç¡®ä¿æ¯ä¸€éƒ¨åˆ†éƒ½æä¾›æ·±åˆ»ä¸”å…·ä½“çš„åˆ†æï¼š

**æˆ‘çš„ç ”ç©¶å…´è¶£:** {self.description}

---

**è®ºæ–‡æ ‡é¢˜:** {paper['title']}
**ä½œè€…:** {', '.join(paper['authors'])}
**ArXiv ID:** {paper['arXiv_id']}
**è®ºæ–‡é“¾æ¥:** {paper['pdf_url']}

---

**è®ºæ–‡å…¨æ–‡:**
```text
{full_text}
```

---

**è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹Markdownæ ¼å¼ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š:**

## 1. {paper['title']}
- **ç›¸å…³æ€§è¯„åˆ†**:  ({paper['relevance_score']}/10)
- **ArXiv ID**: {paper['arXiv_id']}
- **ä½œè€…**: {', '.join(paper['authors'])}
- **è®ºæ–‡é“¾æ¥**: <a href="{paper['pdf_url']}" class="link-btn pdf-link" target="_blank">PDF</a> <a href="{paper['abstract_url']}" class="link-btn arxiv-link" target="_blank">ArXiv</a>
- **ç ”ç©¶èƒŒæ™¯**: [åœ¨è¿™é‡Œè¯¦ç»†é˜è¿°è®ºæ–‡çš„ç ”ç©¶èƒŒæ™¯ã€æ—¨åœ¨è§£å†³çš„å…³é”®é—®é¢˜åŠå…¶é‡è¦æ€§ã€‚]
- **æ–¹æ³•åˆ›æ–°**: [åœ¨è¿™é‡Œæ·±å…¥åˆ†æè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ–¹æ³•ã€æ¨¡å‹æ¶æ„æˆ–ç®—æ³•çš„åˆ›æ–°ä¹‹å¤„ã€‚è¯·å…·ä½“è¯´æ˜å…¶ä¸ç°æœ‰æ–¹æ³•çš„ä¸åŒå’Œä¼˜åŠ¿ã€‚]
- **å®éªŒç»“æœ**: [åœ¨è¿™é‡Œæ€»ç»“è®ºæ–‡çš„å…³é”®å®éªŒè®¾ç½®å’Œä¸»è¦ç»“æœã€‚è¯·æè¿°å®éªŒå¦‚ä½•éªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æåŠå…³é”®çš„æ€§èƒ½æŒ‡æ ‡æˆ–å‘ç°ã€‚]
- **ç»“è®ºæ„ä¹‰**: [åœ¨è¿™é‡Œè¯„ä»·è®ºæ–‡ç»“è®ºçš„ç§‘å­¦æ„ä¹‰ã€æ½œåœ¨åº”ç”¨ä»·å€¼å’Œå¯¹é¢†åŸŸçš„é•¿è¿œå½±å“ã€‚]
- **æ ¸å¿ƒè´¡çŒ®**: [åœ¨è¿™é‡Œç”¨ä¸€æ®µè¯é«˜åº¦æ¦‚æ‹¬è®ºæ–‡æœ€æ ¸å¿ƒã€æœ€ç²¾ç‚¼çš„è´¡çŒ®ã€‚]



**è¯·ç¡®ä¿æœ€ç»ˆæŠ¥å‘Šï¼š**
1.  å®Œå…¨ä½¿ç”¨æµç•…ã€ä¸“ä¸šçš„ä¸­æ–‡æ’°å†™ã€‚
2.  åˆ†ææ·±å…¥ï¼Œé¿å…ç®€å•å¤è¿°åŸæ–‡ã€‚
3.  é€»è¾‘æ¸…æ™°ï¼Œç»“æ„ä¸¥è°¨ï¼Œè§‚ç‚¹ç‹¬åˆ°ã€‚
4.  å¯¹æˆ‘ä¸ªäººçš„ç ”ç©¶å…·æœ‰æ˜ç¡®çš„æŒ‡å¯¼ä»·å€¼ã€‚
5.  è¯·æ³¨æ„ï¼Œæˆ‘çš„ç ”ç©¶å…´è¶£å¯èƒ½æ˜¯ç”¨è‹±æ–‡æè¿°çš„ï¼Œè¯·åœ¨åˆ†ææ—¶å……åˆ†ç†è§£å¹¶å°†å…¶ä¸è®ºæ–‡å†…å®¹å…³è”ã€‚
        """.strip()

    def generate_detailed_paper_analysis(self, paper: Dict[str, Any], temperature: float = None) -> str:
        """ä¸ºå•ç¯‡è®ºæ–‡ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š.
        
        Args:
            paper: åŒ…å«å…¨æ–‡çš„è®ºæ–‡ä¿¡æ¯å­—å…¸
            temperature: ç”Ÿæˆæ¸©åº¦ï¼ˆä¸ºNoneæ—¶ä½¿ç”¨provideré»˜è®¤å€¼ï¼‰
            
        Returns:
            Markdownæ ¼å¼çš„è¯¦ç»†åˆ†ææŠ¥å‘Š
        """
        title_short = paper['title'][:30] + '...' if len(paper['title']) > 30 else paper['title']
        logger.debug(f"è¯¦ç»†åˆ†æç”Ÿæˆå¼€å§‹ - {title_short}")
        
        # æ£€æŸ¥å…¨æ–‡æ˜¯å¦å­˜åœ¨
        if not paper.get("full_text") or len(paper["full_text"]) < 100:
            logger.warning(f"è¯¦ç»†åˆ†æè·³è¿‡ - å…¨æ–‡ä¸å¯ç”¨: {title_short}")
            return f"## {paper['title']}\n- **åˆ†æå¤±è´¥**: æ— æ³•è·å–æœ‰æ•ˆçš„è®ºæ–‡å…¨æ–‡å†…å®¹ã€‚\n"

        prompt = self.build_detailed_analysis_prompt(paper)
        logger.debug(f"è¯¦ç»†åˆ†ææç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")

        try:
            analysis = self.generate_response(prompt, temperature)
            logger.debug(f"è¯¦ç»†åˆ†æç”Ÿæˆå®Œæˆ - {title_short}")
            return analysis
        except Exception as e:
            logger.error(f"è¯¦ç»†åˆ†æç”Ÿæˆå¤±è´¥ - {title_short}: {e}")
            return f"## {paper['title']}\n- **åˆ†æå¤±è´¥**: LLMè°ƒç”¨å‡ºé”™: {e}\n"

    def build_brief_analysis_prompt(self, paper: Dict[str, Any]) -> str:
        """æ„å»ºç®€è¦åˆ†æçš„æç¤ºè¯ã€‚
        
        Args:
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
            
        Returns:
            ç®€è¦åˆ†ææç¤ºè¯
        """
        return f"""
ä½ æ˜¯ä¸€ä½AIç ”ç©¶åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹è®ºæ–‡çš„æ‘˜è¦ï¼Œç”Ÿæˆä¸€ä¸ªç®€æ´çš„ä¸­æ–‡TLDRæ€»ç»“ã€‚

è®ºæ–‡æ ‡é¢˜ï¼š{paper['title']}
è®ºæ–‡æ‘˜è¦ï¼š{paper['abstract']}

è¯·ç”¨1-2å¥è¯æ€»ç»“è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®å’Œä¸»è¦å‘ç°ï¼Œä½¿ç”¨æµç•…çš„ä¸­æ–‡ã€‚
""".strip()

    def generate_brief_analysis(self, paper: Dict[str, Any], temperature: float = None) -> str:
        """ä¸ºå•ç¯‡è®ºæ–‡ç”Ÿæˆç®€è¦åˆ†æï¼ˆTLDRï¼‰ã€‚
        
        Args:
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
            temperature: ç”Ÿæˆæ¸©åº¦ï¼ˆä¸ºNoneæ—¶ä½¿ç”¨provideré»˜è®¤å€¼ï¼‰
            
        Returns:
            ç®€è¦åˆ†æçš„TLDRæ–‡æœ¬
        """
        title_short = paper['title'][:30] + '...' if len(paper['title']) > 30 else paper['title']
        logger.debug(f"ç®€è¦åˆ†æç”Ÿæˆå¼€å§‹ - {title_short}")
        
        prompt = self.build_brief_analysis_prompt(paper)
        
        try:
            tldr = self.generate_response(prompt, temperature)
            logger.debug(f"ç®€è¦åˆ†æç”Ÿæˆå®Œæˆ - {title_short}")
            return tldr.strip()
        except Exception as e:
            logger.error(f"ç®€è¦åˆ†æç”Ÿæˆå¤±è´¥ - {title_short}: {e}")
            return "ç”Ÿæˆæ‘˜è¦å¤±è´¥"


def main():
    """ç‹¬ç«‹æµ‹è¯•å‡½æ•°ã€‚"""""
    import os
    from dotenv import load_dotenv

    # åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
    load_dotenv(os.path.join(os.path.dirname(os.path.dirname(__file__)), '.env'))

    # ä»ç¯å¢ƒå˜é‡è¯»å–é€šä¹‰åƒé—®é…ç½®
    test_model = os.getenv("QWEN_MODEL")
    test_base_url = os.getenv("DASHSCOPE_BASE_URL")
    test_api_key = os.getenv("DASHSCOPE_API_KEY")

    # æ£€æŸ¥ç¯å¢ƒå˜é‡æ˜¯å¦éƒ½å·²è®¾ç½®
    if not all([test_model, test_base_url, test_api_key]):
        logger.error("é”™è¯¯ï¼šè¯·ç¡®ä¿ .env æ–‡ä»¶ä¸­å·²é…ç½® QWEN_MODEL, DASHSCOPE_BASE_URL, å’Œ DASHSCOPE_API_KEY")
        return
    
    logger.debug("æ­£åœ¨ä½¿ç”¨ä»¥ä¸‹é…ç½®è¿›è¡Œæµ‹è¯•ï¼š")
    logger.debug(f"  - æ¨¡å‹: {test_model}")
    logger.debug(f"  - API åœ°å€: {test_base_url}")

    try:
        # åˆå§‹åŒ–æä¾›å•†
        provider = LLMProvider(
            model=test_model,
            base_url=test_base_url,
            api_key=test_api_key
        )

        # æµ‹è¯•ç”Ÿæˆå“åº”
        prompt = "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
        logger.debug(f"\nå‘é€æç¤º: '{prompt}'")
        response = provider.generate_response(prompt)
        logger.success(f"\næ”¶åˆ°å“åº”:\n{response}")
        
        # æµ‹è¯•ä¸åŒæ¸©åº¦è®¾ç½®
        logger.debug("\næµ‹è¯•ä¸åŒæ¸©åº¦è®¾ç½®...")
        creative_prompt = "è¯·åˆ›ä½œä¸€é¦–å…³äºäººå·¥æ™ºèƒ½çš„çŸ­è¯—ã€‚"
        
        logger.debug(f"\nä½æ¸©åº¦ (0.1) å“åº”:")
        low_temp_response = provider.generate_response(creative_prompt, temperature=0.1)
        logger.success(low_temp_response)
        
        logger.debug(f"\né«˜æ¸©åº¦ (0.9) å“åº”:")
        high_temp_response = provider.generate_response(creative_prompt, temperature=0.9)
        logger.success(high_temp_response)

    except Exception as e:
        logger.error(f"\næµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")


def create_light_llm_provider(description: str = "", username: str = "TEST") -> LLMProvider:
    """æ ¹æ®ç¯å¢ƒå˜é‡é…ç½®åˆ›å»ºè½»é‡æ¨¡å‹LLMæä¾›è€…ã€‚
    
    Args:
        description: ç ”ç©¶å…´è¶£æè¿°
        username: ç”¨æˆ·å
        
    Returns:
        é…ç½®å¥½çš„LLMæä¾›è€…å®ä¾‹
    """
    # è·å–è½»é‡æ¨¡å‹æä¾›å•†ç±»å‹
    provider_type = os.getenv('LIGHT_MODEL_PROVIDER', 'qwen').lower()
    
    if provider_type == 'ollama':
        # OLLAMAé…ç½®
        model = os.getenv('OLLAMA_MODEL_LIGHT', 'llama3.2:3b')
        base_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434/v1')
        api_key = 'ollama'  # OLLAMAé€šå¸¸ä¸éœ€è¦çœŸå®çš„APIå¯†é’¥
        temperature = float(os.getenv('OLLAMA_MODEL_LIGHT_TEMPERATURE', '0.7'))
        top_p = float(os.getenv('OLLAMA_MODEL_LIGHT_TOP_P', '0.9'))
        max_tokens = int(os.getenv('OLLAMA_MODEL_LIGHT_MAX_TOKENS', '2000'))
        
        logger.info(f"åˆ›å»ºOLLAMAè½»é‡æ¨¡å‹æä¾›è€… - æ¨¡å‹: {model}, URL: {base_url}")
    else:
        # é€šä¹‰åƒé—®é…ç½®ï¼ˆé»˜è®¤ï¼‰
        model = os.getenv('QWEN_MODEL_LIGHT', 'qwen3-30b-a3b-instruct-2507')
        base_url = os.getenv('DASHSCOPE_BASE_URL', 'https://dashscope.aliyuncs.com/compatible-mode/v1')
        api_key = os.getenv('DASHSCOPE_API_KEY', '')
        temperature = float(os.getenv('QWEN_MODEL_LIGHT_TEMPERATURE', '0.5'))
        top_p = float(os.getenv('QWEN_MODEL_LIGHT_TOP_P', '0.8'))
        max_tokens = int(os.getenv('QWEN_MODEL_LIGHT_MAX_TOKENS', '2000'))
        
        logger.info(f"åˆ›å»ºé€šä¹‰åƒé—®è½»é‡æ¨¡å‹æä¾›è€… - æ¨¡å‹: {model}")
    
    return LLMProvider(
        model=model,
        base_url=base_url,
        api_key=api_key,
        description=description,
        username=username,
        temperature=temperature,
        top_p=top_p,
        max_tokens=max_tokens
    )


if __name__ == "__main__":
    main()
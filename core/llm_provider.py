"""LLMæä¾›å•†æ¨¡å—

ä¸ºè®ºæ–‡åˆ†æå’Œæ€»ç»“æä¾›OpenAIå…¼å®¹APIé›†æˆï¼Œæ”¯æŒé€šä¹‰åƒé—®ã€SiliconFlowã€OLLAMAç­‰ã€‚
åŒæ—¶åŒ…å«LLMæä¾›å•†çš„æŠ½è±¡åŸºç±»å®šä¹‰ã€‚
"""

import time
import json
import traceback
import os
from openai import OpenAI
from typing import Optional, Dict, Any, List
from loguru import logger


class LLMProvider:
    """ç”¨äºLLMäº¤äº’çš„é€šç”¨APIæä¾›å•†ï¼Œæ”¯æŒé€šä¹‰åƒé—®ã€SiliconFlowç­‰OpenAIå…¼å®¹APIã€‚
    è´Ÿè´£æ‰€æœ‰LLMæç¤ºè¯æ„å»ºå’Œäº¤äº’é€»è¾‘ã€‚"""
    
    def __init__(self, model: str, base_url: str, api_key: str, description: str = "", username: str = "TEST", 
                 temperature: float = 0.7, top_p: float = 0.9, max_tokens: int = 4000):
        """åˆå§‹åŒ–LLMæä¾›å•†ã€‚
        
        Args:
            model: æ¨¡å‹åç§°
            base_url: APIåŸºç¡€URL
            api_key: APIå¯†é’¥
            description: ç ”ç©¶å…´è¶£æè¿°
            username: ç”¨æˆ·åï¼Œç”¨äºç”ŸæˆæŠ¥å‘Šæ—¶çš„ç½²å
            temperature: é»˜è®¤æ¸©åº¦å‚æ•°
            top_p: é»˜è®¤top_på‚æ•°
            max_tokens: é»˜è®¤æœ€å¤§tokenæ•°
        """
        logger.info(f"LLMProvideråˆå§‹åŒ–å¼€å§‹")
        self._model_name = model
        self._client = OpenAI(base_url=base_url, api_key=api_key)
        self.description = description
        self.username = username
        self.default_temperature = temperature
        self.default_top_p = top_p
        self.default_max_tokens = max_tokens
        logger.success(f"LLMProvideråˆå§‹åŒ–å®Œæˆ - æ¨¡å‹: {model}, URL: {base_url}, ç”¨æˆ·: {username}, æ¸©åº¦: {temperature}, top_p: {top_p}, max_tokens: {max_tokens}")
    
    @property
    def model_name(self) -> str:
        """è·å–æ¨¡å‹åç§°ã€‚
        
        Returns:
            æ¨¡å‹åç§°å­—ç¬¦ä¸²
        """
        return self._model_name
    
    def _build_messages(self, prompt: str) -> list:
        """æ„å»ºOpenAI APIçš„æ¶ˆæ¯ç»“æ„ã€‚
        
        Args:
            prompt: ç”¨æˆ·æç¤ºæ–‡æœ¬
            
        Returns:
            æ¶ˆæ¯åˆ—è¡¨
        """
        return [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": prompt,
                    }
                ]
            }
        ]
    
    def _call_api_with_retry(
        self, messages: list, temperature: float = None, top_p: float = None, 
        max_tokens: int = None, max_retries: int = 2, wait_time: int = 1
    ) -> str:
        """ä½¿ç”¨é‡è¯•æœºåˆ¶è°ƒç”¨OpenAI APIã€‚
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: ç”Ÿæˆæ¸©åº¦ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
            top_p: top_på‚æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
            max_tokens: æœ€å¤§tokenæ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            wait_time: é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            APIå“åº”å†…å®¹
            
        Raises:
            Exception: APIè°ƒç”¨å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        # ä½¿ç”¨é»˜è®¤å€¼å¦‚æœå‚æ•°ä¸ºNone
        if temperature is None:
            temperature = self.default_temperature
        if top_p is None:
            top_p = self.default_top_p
        if max_tokens is None:
            max_tokens = self.default_max_tokens
            
        logger.debug(f"APIè°ƒç”¨å¼€å§‹ - æ¨¡å‹: {self._model_name}, æ¸©åº¦: {temperature}, top_p: {top_p}, max_tokens: {max_tokens}, æœ€å¤§é‡è¯•: {max_retries}")
        logger.debug(f"APIé…ç½® - å®¢æˆ·ç«¯: {self._client}, åŸºç¡€URL: {self._client.base_url}")
        
        for attempt in range(max_retries):
            try:
                logger.debug(f"ç¬¬ {attempt + 1} æ¬¡APIè°ƒç”¨å°è¯•")
                response = self._client.chat.completions.create(
                    model=self._model_name,
                    messages=messages,
                    temperature=temperature,
                    top_p=top_p,
                    max_tokens=max_tokens,
                )
                logger.debug(f"APIè°ƒç”¨æˆåŠŸ - å°è¯•æ¬¡æ•°: {attempt + 1}")
                return response.choices[0].message.content
                
            except Exception as error:
                error_str = str(error).lower()
                error_type = type(error).__name__
                
                # è¯¦ç»†è®°å½•é”™è¯¯ä¿¡æ¯
                logger.error(f"APIè°ƒç”¨é”™è¯¯è¯¦æƒ…:")
                logger.error(f"  - é”™è¯¯ç±»å‹: {error_type}")
                logger.error(f"  - é”™è¯¯æ¶ˆæ¯: {error}")
                logger.error(f"  - æ¨¡å‹åç§°: {self._model_name}")
                logger.error(f"  - åŸºç¡€URL: {self._client.base_url}")
                logger.error(f"  - å°è¯•æ¬¡æ•°: {attempt + 1}/{max_retries}")
                
                # æ ¹æ®é”™è¯¯ç±»å‹å†³å®šé‡è¯•ç­–ç•¥
                if any(keyword in error_str for keyword in ['rate_limit', '429', 'quota', 'limit']):
                    # APIé™æµé”™è¯¯ï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿
                    wait_time = (attempt + 1) * 3
                    logger.warning(f"APIé™æµ ({attempt + 1}/{max_retries}) - {error}")
                elif any(keyword in error_str for keyword in ['timeout', 'connection', 'network']):
                    # ç½‘ç»œé”™è¯¯ï¼Œçº¿æ€§é€€é¿
                    wait_time = (attempt + 1) * 2
                    logger.warning(f"ç½‘ç»œé”™è¯¯ ({attempt + 1}/{max_retries}) - {error}")
                elif any(keyword in error_str for keyword in ['unauthorized', '401', 'api_key', 'authentication']):
                    # è®¤è¯é”™è¯¯ï¼Œä¸é‡è¯•
                    logger.error(f"APIè®¤è¯é”™è¯¯ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥é…ç½®: {error}")
                    raise
                elif any(keyword in error_str for keyword in ['not found', '404', 'model']):
                    # æ¨¡å‹ä¸å­˜åœ¨é”™è¯¯ï¼Œä¸é‡è¯•
                    logger.error(f"æ¨¡å‹ä¸å­˜åœ¨æˆ–ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥æ¨¡å‹åç§°: {error}")
                    raise
                else:
                    # å…¶ä»–é”™è¯¯ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯åæŠ›å‡º
                    logger.error(f"APIè°ƒç”¨ä¸å¯æ¢å¤é”™è¯¯: {error}")
                    logger.error(f"å®Œæ•´é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                    raise
                    
                if attempt < max_retries - 1:
                    logger.debug(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•")
                    time.sleep(wait_time)
                else:
                    logger.error(f"APIè°ƒç”¨å½»åº•å¤±è´¥ - æ‰€æœ‰ {max_retries} æ¬¡å°è¯•å‡å¤±è´¥")
                    raise
    
    def generate_response(self, prompt: str, temperature: float = None, top_p: float = None, max_tokens: int = None) -> str:
        """ä½¿ç”¨OpenAI APIç”Ÿæˆå“åº”ã€‚
        
        Args:
            prompt: ç”¨æˆ·æç¤ºæ–‡æœ¬
            temperature: ç”Ÿæˆæ¸©åº¦ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
            top_p: top_på‚æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
            max_tokens: æœ€å¤§tokenæ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
            
        Returns:
            ç”Ÿæˆçš„å“åº”æ–‡æœ¬
        """
        messages = self._build_messages(prompt)
        return self._call_api_with_retry(messages, temperature, top_p, max_tokens)
    
    def build_paper_evaluation_prompt(self, paper: Dict[str, Any], description: str) -> str:
        """æ„å»ºè®ºæ–‡è¯„ä¼°æç¤ºè¯ã€‚
        
        Args:
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
            description: ç ”ç©¶å…´è¶£æè¿°
            
        Returns:
            è®ºæ–‡è¯„ä¼°æç¤ºè¯
        """
        return f"""
ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯è®ºæ–‡è¯„ä¼°ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹ç ”ç©¶å…´è¶£æè¿°ï¼Œè¯„ä¼°è¿™ç¯‡è®ºæ–‡çš„ç›¸å…³æ€§ã€‚

ç ”ç©¶å…´è¶£æè¿°ï¼š
{description}

è®ºæ–‡ä¿¡æ¯ï¼š
æ ‡é¢˜ï¼š{paper['title']}
æ‘˜è¦ï¼š{paper['abstract']}
ä½œè€…ï¼š{', '.join(paper['authors'])}
å‘å¸ƒæ—¥æœŸï¼š{paper['published']}

è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼š
{{
    "relevance_score": <0-10çš„æ•°å­—ï¼Œè¡¨ç¤ºç›¸å…³æ€§è¯„åˆ†>,
}}

è¯·ç¡®ä¿è¿”å›çš„æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€‚
        """.strip()
#     è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼š
# {{
#     "relevance_score": <0-10çš„æ•°å­—ï¼Œè¡¨ç¤ºç›¸å…³æ€§è¯„åˆ†>,
#     "research_background": "<ç®€è¦æè¿°è®ºæ–‡çš„ç ”ç©¶èƒŒæ™¯å’Œé—®é¢˜>",
#     "methodology_innovation": "<æè¿°è®ºæ–‡çš„æ–¹æ³•åˆ›æ–°ç‚¹>",
#     "experimental_results": "<æ€»ç»“è®ºæ–‡çš„å®éªŒç»“æœ>",
#     "conclusion_significance": "<è¯„ä»·è®ºæ–‡ç»“è®ºçš„æ„ä¹‰å’Œå½±å“>",
#     "tldr": "<ç”¨ä¸€æ®µè¯æ€»ç»“è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®>"
# }}

    def evaluate_paper_relevance(self, paper: Dict[str, Any], description: str, temperature: float = None) -> Dict[str, Any]:
        """è¯„ä¼°å•ç¯‡è®ºæ–‡çš„ç›¸å…³æ€§ã€‚
        
        Args:
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
            description: ç ”ç©¶å…´è¶£æè¿°
            temperature: ç”Ÿæˆæ¸©åº¦ï¼ˆä¸ºNoneæ—¶ä½¿ç”¨provideré»˜è®¤å€¼ï¼‰
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        title_short = paper['title'][:50] + '...' if len(paper['title']) > 50 else paper['title']
        logger.debug(f"è®ºæ–‡ç›¸å…³æ€§è¯„ä¼°å¼€å§‹ - {title_short}")
        
        prompt = self.build_paper_evaluation_prompt(paper, description)
        
        try:
            response = self.generate_response(prompt, temperature)
            # å°è¯•è§£æJSONå“åº”
            evaluation = json.loads(response)
            
            # ç¡®ä¿ç›¸å…³æ€§è¯„åˆ†å­—æ®µå­˜åœ¨
            if "relevance_score" not in evaluation:
                evaluation["relevance_score"] = 0
            
            # ç¡®ä¿ç›¸å…³æ€§è¯„åˆ†æ˜¯æ•°å­—
            if not isinstance(evaluation["relevance_score"], (int, float)):
                evaluation["relevance_score"] = 0
            
            logger.debug(f"è®ºæ–‡è¯„ä¼°å®Œæˆ - {title_short} (è¯„åˆ†: {evaluation['relevance_score']})")
            return evaluation
            
        except json.JSONDecodeError:
            logger.error(f"JSONè§£æå¤±è´¥ - {title_short}")
            return {
                "relevance_score": 0
            }
        except Exception as e:
            logger.error(f"è®ºæ–‡è¯„ä¼°å¼‚å¸¸ - {title_short}: {e}")
            return {
                "relevance_score": 0
            }
    
    def build_summary_report_prompt(self, papers: List[Dict[str, Any]], current_time: str) -> str:
        """æ„å»ºæ€»ç»“æŠ¥å‘Šæç¤ºè¯ã€‚
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            current_time: å½“å‰æ—¶é—´
            description: ç ”ç©¶å…´è¶£æè¿°
            
        Returns:
            æ€»ç»“æŠ¥å‘Šæç¤ºè¯
        """
        if not papers:
            return ""
        
        # æ„å»ºè®ºæ–‡ä¿¡æ¯
        papers_info = []
        for i, paper in enumerate(papers, 1):
            paper_info = f"""
{i}. **{paper['title']}**
   - ç›¸å…³æ€§è¯„åˆ†: {paper['relevance_score']}/10
   - åŸå§‹æ‘˜è¦: {paper['abstract']}
   - ArXiv ID: {paper['arXiv_id']}
   - å‘å¸ƒæ—¥æœŸ: {paper['published']}

            """.strip()
            papers_info.append(paper_info)
        
        papers_text = "\n\n".join(papers_info)
        
        return f"""
ä½ æ˜¯ä¸€ä½é¡¶å°–çš„AIç ”ç©¶ç§‘å­¦å®¶å’Œèµ„æ·±å­¦æœ¯å¯¼å¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯åŸºäºæˆ‘æä¾›çš„ç ”ç©¶å…´è¶£å’Œæœ€æ–°çš„ArXivè®ºæ–‡åˆ—è¡¨ï¼Œä¸ºæˆ‘ç”Ÿæˆä¸€ä»½é«˜åº¦ç»“æ„åŒ–ã€å¯Œæœ‰æ´å¯ŸåŠ›ä¸”æå…·å®ç”¨ä»·å€¼çš„ä¸­æ–‡ç ”ç©¶åˆ†ææŠ¥å‘Šã€‚

è¯·æ·±å…¥åˆ†ææ¯ç¯‡è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®ï¼Œè¯†åˆ«è®ºæ–‡ä¹‹é—´çš„å†…åœ¨è”ç³»ã€æŠ€æœ¯æ¼”è¿›è¶‹åŠ¿å’Œæ½œåœ¨çš„ç ”ç©¶æœºä¼šã€‚

æˆ‘çš„ç ”ç©¶å…´è¶£: {self.description}

æ¨èè®ºæ–‡åˆ—è¡¨ï¼š
{papers_text}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹Markdownæ¨¡æ¿æ ¼å¼ç”ŸæˆæŠ¥å‘Šï¼Œç¡®ä¿æ¯ä¸€éƒ¨åˆ†éƒ½æä¾›æ·±åˆ»ä¸”å…·ä½“çš„åˆ†æï¼š

# ArXiv ç ”ç©¶æ´å¯ŸæŠ¥å‘Š

> BY:{self.username}
> ({current_time})

##  æ‘˜è¦
[åœ¨æ­¤å¤„æä¾›ä¸€ä¸ªé«˜åº¦æµ“ç¼©çš„æ‰§è¡Œæ‘˜è¦ã€‚ç”¨2-3å¥è¯æ€»ç»“ä»Šå¤©æ‰€æœ‰è®ºæ–‡ä¸­æœ€æ ¸å¿ƒçš„å‘ç°ã€æœ€é‡è¦çš„æŠ€æœ¯è¶‹åŠ¿ï¼Œä»¥åŠä¸æˆ‘çš„ç ”ç©¶å…´è¶£æœ€ç›´æ¥çš„å…³è”ã€‚]

## ğŸ” ä¸»é¢˜æ·±åº¦å‰–æ
[å°†è®ºæ–‡ç²¾å‡†åœ°åˆ’åˆ†åˆ°2-3ä¸ªæ ¸å¿ƒç ”ç©¶ä¸»é¢˜ã€‚å¯¹äºæ¯ä¸ªä¸»é¢˜ï¼Œè¿›è¡Œæ·±å…¥åˆ†æï¼š]

### ä¸»é¢˜ä¸€ï¼š[ä¸»é¢˜åç§°ï¼Œä¾‹å¦‚ï¼šå¤šæ¨¡æ€å¤§æ¨¡å‹çš„é²æ£’æ€§ä¸æ³›åŒ–]
* **æ ¸å¿ƒé—®é¢˜ (Problem Domain):** è¯¥ä¸»é¢˜ä¸‹çš„è®ºæ–‡ä¸»è¦è‡´åŠ›äºè§£å†³ä»€ä¹ˆå…³é”®ç§‘å­¦æˆ–å·¥ç¨‹é—®é¢˜ï¼Ÿ
* **ä»£è¡¨æ€§è®ºæ–‡ (Key Papers):** [åˆ—å‡ºè¯¥ä¸»é¢˜ä¸‹çš„1-3ç¯‡å…³é”®è®ºæ–‡çš„æ ‡é¢˜]
* **æ–¹æ³•è®ºåˆ›æ–° (Methodological Innovations):**
    * **[è®ºæ–‡Aåç§°]:** [ç®€è¿°å…¶æ ¸å¿ƒæ–¹æ³•ã€æ¨¡å‹æ¶æ„æˆ–ç®—æ³•çš„åˆ›æ–°ä¹‹å¤„ã€‚]
    * **[è®ºæ–‡Båç§°]:** [ç®€è¿°å…¶æ ¸å¿ƒæ–¹æ³•ã€æ¨¡å‹æ¶æ„æˆ–ç®—æ³•çš„åˆ›æ–°ä¹‹å¤„ã€‚]
* **ç ”ç©¶å¯ç¤º (Insights & Implications):** è¿™äº›æˆæœçš„ç ”ç©¶æˆæœå¯¹è¯¥é¢†åŸŸæ„å‘³ç€ä»€ä¹ˆï¼Ÿå®ƒä»¬éªŒè¯äº†ä»€ä¹ˆå‡è®¾ï¼Œæˆ–è€…æ¨ç¿»äº†ä»€ä¹ˆä¼ ç»Ÿè®¤çŸ¥ï¼Ÿ

### ä¸»é¢˜äºŒï¼š[ä¸»é¢˜åç§°ï¼Œä¾‹å¦‚ï¼šAgentçš„è‡ªä¸»å­¦ä¹ ä¸è¿›åŒ–]
* **æ ¸å¿ƒé—®é¢˜ (Problem Domain):** è¯¥ä¸»é¢˜ä¸‹çš„è®ºæ–‡ä¸»è¦è‡´åŠ›äºè§£å†³ä»€ä¹ˆå…³é”®ç§‘å­¦æˆ–å·¥ç¨‹é—®é¢˜ï¼Ÿ
* **ä»£è¡¨æ€§è®ºæ–‡ (Key Papers):** [åˆ—å‡ºè¯¥ä¸»é¢˜ä¸‹çš„1-3ç¯‡å…³é”®è®ºæ–‡çš„æ ‡é¢˜]
* **æ–¹æ³•è®ºåˆ›æ–° (Methodological Innovations):**
    * **[è®ºæ–‡Aåç§°]:** [ç®€è¿°å…¶æ ¸å¿ƒæ–¹æ³•ã€æ¨¡å‹æ¶æ„æˆ–ç®—æ³•çš„åˆ›æ–°ä¹‹å¤„ã€‚]
* **ç ”ç©¶å¯ç¤º (Insights & Implications):** è¿™äº›æˆæœçš„ç ”ç©¶æˆæœå¯¹è¯¥é¢†åŸŸæ„å‘³ç€ä»€ä¹ˆï¼Ÿå®ƒä»¬éªŒè¯äº†ä»€ä¹ˆå‡è®¾ï¼Œæˆ–è€…æ¨ç¿»äº†ä»€ä¹ˆä¼ ç»Ÿè®¤çŸ¥ï¼Ÿ


## ğŸ“ˆ å®è§‚è¶‹åŠ¿ä¸å‰ç»
[ç»¼åˆæ‰€æœ‰è®ºæ–‡ï¼Œä»æ›´é«˜ç»´åº¦è¿›è¡Œåˆ†æï¼š]
* **æŠ€æœ¯è¶‹åŠ¿ (Tech Trends):** å½“å‰ç ”ç©¶æœ€çƒ­é—¨çš„æŠ€æœ¯æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆä¾‹å¦‚ï¼šä»æ¨¡å‹å¾®è°ƒè½¬å‘è‡ªä¸»å­¦ä¹ ã€å¯¹ç‰¹å®šé¢†åŸŸï¼ˆå¦‚é‡‘èï¼‰çš„æ·±å…¥åº”ç”¨ç­‰ï¼‰
* **æ½œåœ¨æœºä¼š (Opportunities):** åŸºäºç°æœ‰ç ”ç©¶ï¼Œå“ªäº›é—®é¢˜å°šæœªè¢«è§£å†³ï¼Ÿå­˜åœ¨å“ªäº›æ–°çš„ç ”ç©¶ç©ºç™½æˆ–äº¤å‰é¢†åŸŸæœºä¼šï¼Ÿ
* **å€¼å¾—å…³æ³¨çš„å·¥å…·/æ•°æ®é›† (Noteworthy Tools/Datasets):** æœ¬æ¬¡æ¨èä¸­æ˜¯å¦å‡ºç°äº†æ–°çš„ã€æœ‰æ½œåŠ›çš„åŸºå‡†æµ‹è¯•ã€æ•°æ®é›†æˆ–å¼€æºå·¥å…·ï¼Ÿ


## ğŸ’¡ ä¸ªæ€§åŒ–å»ºè®®ä¸è¡ŒåŠ¨æŒ‡å—
[æœ¬éƒ¨åˆ†å°†åˆ†æä¸æˆ‘çš„ç ”ç©¶å…´è¶£ç´§å¯†ç»“åˆï¼Œæä¾›å¯æ“ä½œçš„å»ºè®®ï¼š]
* **å…³è”æ€§è§£è¯» (Relevance Analysis):** å…·ä½“è¯´æ˜ä»Šå¤©çš„å“ªäº›è®ºæ–‡/æŠ€æœ¯ï¼ˆä¾‹å¦‚ `SEAgent` çš„è‡ªä¸»å­¦ä¹ æ¡†æ¶ï¼Œæˆ– `FinMMR` çš„è¯„æµ‹æ–¹æ³•ï¼‰ä¸æˆ‘çš„ç ”ç©¶æ–¹å‘ç›´æ¥ç›¸å…³ã€‚
* **å¯å€Ÿé‰´ç‚¹ (Actionable Takeaways):** æˆ‘å¯ä»¥ä»è¿™äº›è®ºæ–‡ä¸­å€Ÿé‰´å“ªäº›å…·ä½“çš„æŠ€æœ¯ã€å®éªŒè®¾è®¡æˆ–åˆ†ææ€è·¯æ¥æ”¹è¿›æˆ‘è‡ªå·±çš„ç ”ç©¶é¡¹ç›®ï¼Ÿ
* **ä¼˜å…ˆé˜…è¯»å»ºè®® (Reading Priority):** åŸºäºç›¸å…³æ€§å’Œåˆ›æ–°æ€§ï¼Œå»ºè®®æˆ‘ä¼˜å…ˆç²¾è¯»å“ª1-2ç¯‡è®ºæ–‡ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ

---

**è¯·ç¡®ä¿æœ€ç»ˆæŠ¥å‘Šï¼š**
1.  å®Œå…¨ä½¿ç”¨æµç•…ã€ä¸“ä¸šçš„ä¸­æ–‡æ’°å†™ã€‚
2.  åˆ†ææ·±å…¥ï¼Œé¿å…ç®€å•å¤è¿°æ‘˜è¦ã€‚
3.  é€»è¾‘æ¸…æ™°ï¼Œç»“æ„ä¸¥è°¨ï¼Œè§‚ç‚¹ç‹¬åˆ°ã€‚
4.  å¯¹æˆ‘ä¸ªäººçš„ç ”ç©¶å…·æœ‰æ˜ç¡®çš„æŒ‡å¯¼ä»·å€¼ã€‚
5.  è¯·æ³¨æ„ï¼Œæˆ‘çš„ç ”ç©¶å…´è¶£å¯èƒ½æ˜¯ç”¨è‹±æ–‡æè¿°çš„ï¼Œè¯·åœ¨åˆ†ææ—¶å……åˆ†ç†è§£å¹¶å°†å…¶ä¸è®ºæ–‡å†…å®¹å…³è”ã€‚
        """.strip()



    def generate_summary_report(self, papers: List[Dict[str, Any]], current_time: str, temperature: float = None) -> str:
        """ç”Ÿæˆè®ºæ–‡æ¨èçš„Markdownæ€»ç»“æŠ¥å‘Šã€‚
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨ï¼ˆå·²æ’åºï¼‰
            current_time: å½“å‰æ—¶é—´
            temperature: ç”Ÿæˆæ¸©åº¦ï¼ˆä¸ºNoneæ—¶ä½¿ç”¨provideré»˜è®¤å€¼ï¼‰
            
        Returns:
            Markdownæ ¼å¼çš„æ€»ç»“æŠ¥å‘Š
        """
        if not papers:
            logger.warning("æ€»ç»“æŠ¥å‘Šè·³è¿‡ - æ— æ¨èè®ºæ–‡")
            return "ä»Šæ—¥æ— æ¨èè®ºæ–‡ã€‚"
        
        logger.info(f"æ€»ç»“æŠ¥å‘Šç”Ÿæˆå¼€å§‹ - åŸå§‹è®ºæ–‡: {len(papers)} ç¯‡")
        
        # åŠ¨æ€é€‰æ‹©æœ€ä½³è®ºæ–‡æ•°é‡ï¼Œç¡®ä¿æç¤ºè¯é•¿åº¦ä¸è¶…è¿‡15000å­—ç¬¦
        optimal_papers = self._select_optimal_papers_for_prompt(papers, current_time, max_length=30000)
        
        logger.debug(f"è®ºæ–‡æ•°é‡ä¼˜åŒ–å®Œæˆ - æœ€ç»ˆé€‰æ‹©: {len(optimal_papers)} ç¯‡")
        
        prompt = self.build_summary_report_prompt(optimal_papers, current_time)
        logger.debug(f"æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
        
        try:
            logger.debug("LLMæ€»ç»“ç”Ÿæˆå¼€å§‹")
            start_time = time.time()
            summary = self.generate_response(prompt, temperature)
            end_time = time.time()
            logger.success(f"æ€»ç»“æŠ¥å‘Šç”Ÿæˆå®Œæˆ - è€—æ—¶: {end_time - start_time:.2f}ç§’, é•¿åº¦: {len(summary)} å­—ç¬¦")
            return summary
        except Exception as e:
            logger.error(f"æ€»ç»“æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return "ç”Ÿæˆæ€»ç»“å¤±è´¥ã€‚"

    def _select_optimal_papers_for_prompt(self, papers: List[Dict[str, Any]], current_time: str, max_length: int = 15000) -> List[Dict[str, Any]]:
        """æ ¹æ®æç¤ºè¯é•¿åº¦é™åˆ¶åŠ¨æ€é€‰æ‹©æœ€ä½³è®ºæ–‡æ•°é‡ã€‚
        
        Args:
            papers: å·²æ’åºçš„è®ºæ–‡åˆ—è¡¨ï¼ˆæŒ‰ç›¸å…³æ€§ä»é«˜åˆ°ä½ï¼‰
            current_time: å½“å‰æ—¶é—´
            max_length: æç¤ºè¯æœ€å¤§é•¿åº¦é™åˆ¶
            
        Returns:
            ä¼˜åŒ–åçš„è®ºæ–‡åˆ—è¡¨
        """
        if not papers:
            return papers
        
        logger.debug(f"è®ºæ–‡æ•°é‡ä¼˜åŒ–å¼€å§‹ - æœ€å¤§é•¿åº¦é™åˆ¶: {max_length} å­—ç¬¦")
        
        # ä»1ç¯‡è®ºæ–‡å¼€å§‹é€æ­¥å¢åŠ ï¼Œæ‰¾åˆ°æœ€ä½³æ•°é‡
        optimal_papers = []
        
        for i in range(1, len(papers) + 1):
            candidate_papers = papers[:i]
            test_prompt = self.build_summary_report_prompt(candidate_papers, current_time)
            
            if len(test_prompt) <= max_length:
                optimal_papers = candidate_papers
                logger.debug(f"è®ºæ–‡æ•°é‡æµ‹è¯•é€šè¿‡ - {i} ç¯‡ (é•¿åº¦: {len(test_prompt)} å­—ç¬¦)")
            else:
                logger.debug(f"è®ºæ–‡æ•°é‡è¾¾åˆ°ä¸Šé™ - {i} ç¯‡è¶…å‡ºé™åˆ¶ (é•¿åº¦: {len(test_prompt)} å­—ç¬¦)")
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„è®ºæ–‡æ•°é‡ï¼ˆè¿1ç¯‡éƒ½è¶…é•¿ï¼‰ï¼Œè‡³å°‘è¿”å›ç¬¬1ç¯‡
        if not optimal_papers and papers:
            optimal_papers = papers[:1]
            logger.warning("å¼ºåˆ¶é€‰æ‹©1ç¯‡è®ºæ–‡ - å³ä½¿å¯èƒ½è¶…å‡ºé•¿åº¦é™åˆ¶")
        
        return optimal_papers


    def build_detailed_analysis_prompt(self, paper: Dict[str, Any]) -> str:
        """æ„å»ºå•ç¯‡è®ºæ–‡çš„è¯¦ç»†åˆ†ææç¤ºè¯.
        
        Args:
            paper: åŒ…å«å…¨æ–‡çš„è®ºæ–‡ä¿¡æ¯å­—å…¸
            
        Returns:
            è¯¦ç»†åˆ†ææç¤ºè¯
        """
        # Truncate full_text to avoid API errors
        full_text = paper.get('full_text', '')
        if len(full_text) > 15000:
            full_text = full_text[:15000] + "... (truncated)"

        return f"""
ä½ æ˜¯ä¸€ä½é¡¶å°–çš„AIç ”ç©¶ç§‘å­¦å®¶å’Œèµ„æ·±å­¦æœ¯å¯¼å¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯åŸºäºæˆ‘æä¾›çš„ç ”ç©¶å…´è¶£å’Œä¸€ç¯‡å®Œæ•´çš„ArXivè®ºæ–‡ï¼Œä¸ºæˆ‘ç”Ÿæˆä¸€ä»½é«˜åº¦ç»“æ„åŒ–ã€å¯Œæœ‰æ´å¯ŸåŠ›çš„ä¸­æ–‡ç ”ç©¶åˆ†ææŠ¥å‘Šã€‚

è¯·æ·±å…¥åˆ†æè¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹Markdownæ¨¡æ¿æ ¼å¼ç”ŸæˆæŠ¥å‘Šï¼Œç¡®ä¿æ¯ä¸€éƒ¨åˆ†éƒ½æä¾›æ·±åˆ»ä¸”å…·ä½“çš„åˆ†æï¼š

**æˆ‘çš„ç ”ç©¶å…´è¶£:** {self.description}

---

**è®ºæ–‡æ ‡é¢˜:** {paper['title']}
**ä½œè€…:** {', '.join(paper['authors'])}
**ArXiv ID:** {paper['arXiv_id']}
**è®ºæ–‡é“¾æ¥:** {paper['pdf_url']}

---

**è®ºæ–‡å…¨æ–‡:**
```text
{full_text}
```

---

**è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹Markdownæ ¼å¼ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š:**

## 1. {paper['title']}
- **ç›¸å…³æ€§è¯„åˆ†**: â­â­â­â­â­ ({paper['relevance_score']}/10)
- **ArXiv ID**: {paper['arXiv_id']}
- **ä½œè€…**: {', '.join(paper['authors'])}
- **è®ºæ–‡é“¾æ¥**: <a href="{paper['pdf_url']}" class="link-btn pdf-link" target="_blank">PDF</a> <a href="{paper['abstract_url']}" class="link-btn arxiv-link" target="_blank">ArXiv</a>
- **ç ”ç©¶èƒŒæ™¯**: [åœ¨è¿™é‡Œè¯¦ç»†é˜è¿°è®ºæ–‡çš„ç ”ç©¶èƒŒæ™¯ã€æ—¨åœ¨è§£å†³çš„å…³é”®é—®é¢˜åŠå…¶é‡è¦æ€§ã€‚]
- **æ–¹æ³•åˆ›æ–°**: [åœ¨è¿™é‡Œæ·±å…¥åˆ†æè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ–¹æ³•ã€æ¨¡å‹æ¶æ„æˆ–ç®—æ³•çš„åˆ›æ–°ä¹‹å¤„ã€‚è¯·å…·ä½“è¯´æ˜å…¶ä¸ç°æœ‰æ–¹æ³•çš„ä¸åŒå’Œä¼˜åŠ¿ã€‚]
- **å®éªŒç»“æœ**: [åœ¨è¿™é‡Œæ€»ç»“è®ºæ–‡çš„å…³é”®å®éªŒè®¾ç½®å’Œä¸»è¦ç»“æœã€‚è¯·æè¿°å®éªŒå¦‚ä½•éªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æåŠå…³é”®çš„æ€§èƒ½æŒ‡æ ‡æˆ–å‘ç°ã€‚]
- **ç»“è®ºæ„ä¹‰**: [åœ¨è¿™é‡Œè¯„ä»·è®ºæ–‡ç»“è®ºçš„ç§‘å­¦æ„ä¹‰ã€æ½œåœ¨åº”ç”¨ä»·å€¼å’Œå¯¹é¢†åŸŸçš„é•¿è¿œå½±å“ã€‚]
- **æ ¸å¿ƒè´¡çŒ®**: [åœ¨è¿™é‡Œç”¨ä¸€æ®µè¯é«˜åº¦æ¦‚æ‹¬è®ºæ–‡æœ€æ ¸å¿ƒã€æœ€ç²¾ç‚¼çš„è´¡çŒ®ã€‚]



**è¯·ç¡®ä¿æœ€ç»ˆæŠ¥å‘Šï¼š**
1.  å®Œå…¨ä½¿ç”¨æµç•…ã€ä¸“ä¸šçš„ä¸­æ–‡æ’°å†™ã€‚
2.  åˆ†ææ·±å…¥ï¼Œé¿å…ç®€å•å¤è¿°åŸæ–‡ã€‚
3.  é€»è¾‘æ¸…æ™°ï¼Œç»“æ„ä¸¥è°¨ï¼Œè§‚ç‚¹ç‹¬åˆ°ã€‚
4.  å¯¹æˆ‘ä¸ªäººçš„ç ”ç©¶å…·æœ‰æ˜ç¡®çš„æŒ‡å¯¼ä»·å€¼ã€‚
5.  è¯·æ³¨æ„ï¼Œæˆ‘çš„ç ”ç©¶å…´è¶£å¯èƒ½æ˜¯ç”¨è‹±æ–‡æè¿°çš„ï¼Œè¯·åœ¨åˆ†ææ—¶å……åˆ†ç†è§£å¹¶å°†å…¶ä¸è®ºæ–‡å†…å®¹å…³è”ã€‚
        """.strip()

    def generate_detailed_paper_analysis(self, paper: Dict[str, Any], temperature: float = None) -> str:
        """ä¸ºå•ç¯‡è®ºæ–‡ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š.
        
        Args:
            paper: åŒ…å«å…¨æ–‡çš„è®ºæ–‡ä¿¡æ¯å­—å…¸
            temperature: ç”Ÿæˆæ¸©åº¦ï¼ˆä¸ºNoneæ—¶ä½¿ç”¨provideré»˜è®¤å€¼ï¼‰
            
        Returns:
            Markdownæ ¼å¼çš„è¯¦ç»†åˆ†ææŠ¥å‘Š
        """
        title_short = paper['title'][:30] + '...' if len(paper['title']) > 30 else paper['title']
        logger.debug(f"è¯¦ç»†åˆ†æç”Ÿæˆå¼€å§‹ - {title_short}")
        
        # æ£€æŸ¥å…¨æ–‡æ˜¯å¦å­˜åœ¨
        if not paper.get("full_text") or len(paper["full_text"]) < 100:
            logger.warning(f"è¯¦ç»†åˆ†æè·³è¿‡ - å…¨æ–‡ä¸å¯ç”¨: {title_short}")
            return f"## {paper['title']}\n- **åˆ†æå¤±è´¥**: æ— æ³•è·å–æœ‰æ•ˆçš„è®ºæ–‡å…¨æ–‡å†…å®¹ã€‚\n"

        prompt = self.build_detailed_analysis_prompt(paper)
        logger.debug(f"è¯¦ç»†åˆ†ææç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")

        try:
            analysis = self.generate_response(prompt, temperature)
            logger.debug(f"è¯¦ç»†åˆ†æç”Ÿæˆå®Œæˆ - {title_short}")
            return analysis
        except Exception as e:
            logger.error(f"è¯¦ç»†åˆ†æç”Ÿæˆå¤±è´¥ - {title_short}: {e}")
            return f"## {paper['title']}\n- **åˆ†æå¤±è´¥**: LLMè°ƒç”¨å‡ºé”™: {e}\n"

    def build_brief_analysis_prompt(self, paper: Dict[str, Any]) -> str:
        """æ„å»ºç®€è¦åˆ†æçš„æç¤ºè¯ã€‚
        
        Args:
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
            
        Returns:
            ç®€è¦åˆ†ææç¤ºè¯
        """
        return f"""
ä½ æ˜¯ä¸€ä½AIç ”ç©¶åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹è®ºæ–‡çš„æ‘˜è¦ï¼Œç”Ÿæˆä¸€ä¸ªç®€æ´çš„ä¸­æ–‡TLDRæ€»ç»“ã€‚

è®ºæ–‡æ ‡é¢˜ï¼š{paper['title']}
è®ºæ–‡æ‘˜è¦ï¼š{paper['abstract']}

è¯·ç”¨1-2å¥è¯æ€»ç»“è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®å’Œä¸»è¦å‘ç°ï¼Œä½¿ç”¨æµç•…çš„ä¸­æ–‡ã€‚
""".strip()

    def generate_brief_analysis(self, paper: Dict[str, Any], temperature: float = None) -> str:
        """ä¸ºå•ç¯‡è®ºæ–‡ç”Ÿæˆç®€è¦åˆ†æï¼ˆTLDRï¼‰ã€‚
        
        Args:
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
            temperature: ç”Ÿæˆæ¸©åº¦ï¼ˆä¸ºNoneæ—¶ä½¿ç”¨provideré»˜è®¤å€¼ï¼‰
            
        Returns:
            ç®€è¦åˆ†æçš„TLDRæ–‡æœ¬
        """
        title_short = paper['title'][:30] + '...' if len(paper['title']) > 30 else paper['title']
        logger.debug(f"ç®€è¦åˆ†æç”Ÿæˆå¼€å§‹ - {title_short}")
        
        prompt = self.build_brief_analysis_prompt(paper)
        
        try:
            tldr = self.generate_response(prompt, temperature)
            logger.debug(f"ç®€è¦åˆ†æç”Ÿæˆå®Œæˆ - {title_short}")
            return tldr.strip()
        except Exception as e:
            logger.error(f"ç®€è¦åˆ†æç”Ÿæˆå¤±è´¥ - {title_short}: {e}")
            return "ç”Ÿæˆæ‘˜è¦å¤±è´¥"


def main():
    """ç‹¬ç«‹æµ‹è¯•å‡½æ•°ã€‚"""""
    import os
    from dotenv import load_dotenv

    # åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
    load_dotenv(os.path.join(os.path.dirname(os.path.dirname(__file__)), '.env'))

    # ä»ç¯å¢ƒå˜é‡è¯»å–é€šä¹‰åƒé—®é…ç½®
    test_model = os.getenv("QWEN_MODEL")
    test_base_url = os.getenv("DASHSCOPE_BASE_URL")
    test_api_key = os.getenv("DASHSCOPE_API_KEY")

    # æ£€æŸ¥ç¯å¢ƒå˜é‡æ˜¯å¦éƒ½å·²è®¾ç½®
    if not all([test_model, test_base_url, test_api_key]):
        logger.error("é”™è¯¯ï¼šè¯·ç¡®ä¿ .env æ–‡ä»¶ä¸­å·²é…ç½® QWEN_MODEL, DASHSCOPE_BASE_URL, å’Œ DASHSCOPE_API_KEY")
        return
    
    logger.debug("æ­£åœ¨ä½¿ç”¨ä»¥ä¸‹é…ç½®è¿›è¡Œæµ‹è¯•ï¼š")
    logger.debug(f"  - æ¨¡å‹: {test_model}")
    logger.debug(f"  - API åœ°å€: {test_base_url}")

    try:
        # åˆå§‹åŒ–æä¾›å•†
        provider = LLMProvider(
            model=test_model,
            base_url=test_base_url,
            api_key=test_api_key
        )

        # æµ‹è¯•ç”Ÿæˆå“åº”
        prompt = "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
        logger.debug(f"\nå‘é€æç¤º: '{prompt}'")
        response = provider.generate_response(prompt)
        logger.success(f"\næ”¶åˆ°å“åº”:\n{response}")
        
        # æµ‹è¯•ä¸åŒæ¸©åº¦è®¾ç½®
        logger.debug("\næµ‹è¯•ä¸åŒæ¸©åº¦è®¾ç½®...")
        creative_prompt = "è¯·åˆ›ä½œä¸€é¦–å…³äºäººå·¥æ™ºèƒ½çš„çŸ­è¯—ã€‚"
        
        logger.debug(f"\nä½æ¸©åº¦ (0.1) å“åº”:")
        low_temp_response = provider.generate_response(creative_prompt, temperature=0.1)
        logger.success(low_temp_response)
        
        logger.debug(f"\né«˜æ¸©åº¦ (0.9) å“åº”:")
        high_temp_response = provider.generate_response(creative_prompt, temperature=0.9)
        logger.success(high_temp_response)

    except Exception as e:
        logger.error(f"\næµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")


def create_light_llm_provider(description: str = "", username: str = "TEST") -> LLMProvider:
    """æ ¹æ®ç¯å¢ƒå˜é‡é…ç½®åˆ›å»ºè½»é‡æ¨¡å‹LLMæä¾›è€…ã€‚
    
    Args:
        description: ç ”ç©¶å…´è¶£æè¿°
        username: ç”¨æˆ·å
        
    Returns:
        é…ç½®å¥½çš„LLMæä¾›è€…å®ä¾‹
    """
    # è·å–è½»é‡æ¨¡å‹æä¾›å•†ç±»å‹
    provider_type = os.getenv('LIGHT_MODEL_PROVIDER', 'qwen').lower()
    
    if provider_type == 'ollama':
        # OLLAMAé…ç½®
        model = os.getenv('OLLAMA_MODEL_LIGHT', 'llama3.2:3b')
        base_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434/v1')
        api_key = 'ollama'  # OLLAMAé€šå¸¸ä¸éœ€è¦çœŸå®çš„APIå¯†é’¥
        temperature = float(os.getenv('OLLAMA_MODEL_LIGHT_TEMPERATURE', '0.7'))
        top_p = float(os.getenv('OLLAMA_MODEL_LIGHT_TOP_P', '0.9'))
        max_tokens = int(os.getenv('OLLAMA_MODEL_LIGHT_MAX_TOKENS', '2000'))
        
        logger.info(f"åˆ›å»ºOLLAMAè½»é‡æ¨¡å‹æä¾›è€… - æ¨¡å‹: {model}, URL: {base_url}")
    else:
        # é€šä¹‰åƒé—®é…ç½®ï¼ˆé»˜è®¤ï¼‰
        model = os.getenv('QWEN_MODEL_LIGHT', 'qwen3-30b-a3b-instruct-2507')
        base_url = os.getenv('DASHSCOPE_BASE_URL', 'https://dashscope.aliyuncs.com/compatible-mode/v1')
        api_key = os.getenv('DASHSCOPE_API_KEY', '')
        temperature = float(os.getenv('QWEN_MODEL_LIGHT_TEMPERATURE', '0.5'))
        top_p = float(os.getenv('QWEN_MODEL_LIGHT_TOP_P', '0.8'))
        max_tokens = int(os.getenv('QWEN_MODEL_LIGHT_MAX_TOKENS', '2000'))
        
        logger.info(f"åˆ›å»ºé€šä¹‰åƒé—®è½»é‡æ¨¡å‹æä¾›è€… - æ¨¡å‹: {model}")
    
    return LLMProvider(
        model=model,
        base_url=base_url,
        api_key=api_key,
        description=description,
        username=username,
        temperature=temperature,
        top_p=top_p,
        max_tokens=max_tokens
    )


if __name__ == "__main__":
    main()
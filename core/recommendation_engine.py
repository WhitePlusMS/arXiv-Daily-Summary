"""æ¨èå¼•æ“æ¨¡å—

æä¾›è®ºæ–‡æ¨èçš„æ ¸å¿ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬ä»ArXivè·å–è®ºæ–‡ã€ä½¿ç”¨LLMè¯„ä¼°ç›¸å…³æ€§ã€ç”Ÿæˆæ¨èæŠ¥å‘Šç­‰ã€‚
"""

import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any, Optional
from datetime import datetime
from loguru import logger

from .arxiv_fetcher import ArxivFetcher
from .llm_provider import LLMProvider, create_light_llm_provider


class RecommendationEngine:
    """è®ºæ–‡æ¨èå¼•æ“ï¼Œè´Ÿè´£è·å–ã€è¯„ä¼°å’Œæ¨èArXivè®ºæ–‡ã€‚"""

    def __init__(
        self,
        categories: List[str],
        max_entries: int,
        num_detailed_papers: int,
        num_brief_papers: int,
        model: str,
        base_url: str,
        api_key: str,
        description: str,
        username: str = "TEST",
        num_workers: int = 2,
        temperature: float = 0.7,
        top_p: float = 0.9,
        max_tokens: int = 4000,
        arxiv_fetcher: Optional[ArxivFetcher] = None,
        llm_provider: Optional[LLMProvider] = None,
        light_llm_provider: Optional[LLMProvider] = None,
    ):
        """åˆå§‹åŒ–æ¨èå¼•æ“ã€‚
        
        Args:
            categories: ArXivåˆ†ç±»åˆ—è¡¨
            max_entries: æ¯ä¸ªåˆ†ç±»è·å–çš„æœ€å¤§è®ºæ–‡æ•°
            num_detailed_papers: è¯¦ç»†åˆ†æçš„è®ºæ–‡æ•°
            num_brief_papers: ç®€è¦åˆ†æçš„è®ºæ–‡æ•°
            model: LLMæ¨¡å‹åç§°
            base_url: LLM APIåŸºç¡€URL
            api_key: LLM APIå¯†é’¥
            description: ç ”ç©¶å…´è¶£æè¿°
            username: ç”¨æˆ·åï¼Œç”¨äºç”ŸæˆæŠ¥å‘Šæ—¶çš„ç½²å
            num_workers: å¹¶è¡Œå¤„ç†çº¿ç¨‹æ•°
            temperature: LLMç”Ÿæˆæ¸©åº¦
            top_p: LLM top_på‚æ•°
            max_tokens: LLMæœ€å¤§tokenæ•°
        """
        logger.info("æ¨èå¼•æ“åˆå§‹åŒ–å¼€å§‹")
        self.categories = categories
        self.max_entries = max_entries
        self.num_detailed_papers = num_detailed_papers
        self.num_brief_papers = num_brief_papers
        self.description = description
        self.num_workers = num_workers
        
        # åˆå§‹åŒ–ArXivè·å–å™¨å’ŒLLMæä¾›å•†ï¼ˆæ”¯æŒä¾èµ–æ³¨å…¥ï¼Œå‡å°‘é‡å¤æ„é€ ä¸è€¦åˆï¼‰
        logger.debug("åˆå§‹åŒ–ArXivè·å–å™¨å’ŒLLMæä¾›å•†")
        self.arxiv_fetcher = arxiv_fetcher or ArxivFetcher()

        # ä¸»LLMæä¾›è€…ï¼ˆç”¨äºè¯¦ç»†åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆï¼‰
        self.llm_provider = llm_provider or LLMProvider(
            model=model,
            base_url=base_url,
            api_key=api_key,
            description=description,
            username=username,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
        )

        # è½»é‡æ¨¡å‹æä¾›è€…ï¼ˆç”¨äºè®ºæ–‡ç›¸å…³æ€§è¯„ä¼°ï¼‰
        self.light_llm_provider = light_llm_provider or create_light_llm_provider(
            description=description,
            username=username,
        )
        
        self.temperature = temperature
        self.top_p = top_p
        self.max_tokens = max_tokens
        
        logger.success(f"æ¨èå¼•æ“åˆå§‹åŒ–å®Œæˆ - åˆ†ç±»: {categories}, è¯¦ç»†åˆ†æ: {num_detailed_papers}, ç®€è¦åˆ†æ: {num_brief_papers}")

    def _fetch_papers_from_categories(self, date: str = None) -> List[Dict[str, Any]]:
        """ä»æ‰€æœ‰æŒ‡å®šåˆ†ç±»ä¸­è·å–è®ºæ–‡ã€‚
        
        Args:
            date: æŒ‡å®šæ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DDï¼Œå¦‚æœä¸ºNoneåˆ™è·å–æœ€æ–°è®ºæ–‡
        """
        logger.info(f"è®ºæ–‡è·å–å¼€å§‹ - {len(self.categories)} ä¸ªåˆ†ç±»")
        all_papers = []
        
        def fetch_category_papers(category: str) -> List[Dict[str, Any]]:
            """è·å–å•ä¸ªåˆ†ç±»çš„è®ºæ–‡ã€‚"""
            logger.debug(f"è·å–åˆ†ç±» {category} çš„è®ºæ–‡")
            if date:
                # ä½¿ç”¨åŸºäºæ—¥æœŸçš„åˆ†é¡µè·å–
                papers = self.arxiv_fetcher.fetch_papers_paged(
                    category.strip(), 
                    date, 
                    per_page=min(self.max_entries, 200), 
                    max_pages=5,
                    max_total=self.max_entries
                )
                logger.debug(f"åˆ†ç±» {category} ({date}): {len(papers)} ç¯‡è®ºæ–‡")
            else:
                # ä½¿ç”¨åŸæœ‰çš„è·å–æ–¹å¼
                papers = self.arxiv_fetcher.fetch_papers(category.strip(), self.max_entries)
                logger.debug(f"åˆ†ç±» {category}: {len(papers)} ç¯‡è®ºæ–‡")
            return papers

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œè·å–ä¸åŒåˆ†ç±»çš„è®ºæ–‡
        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
            future_to_category = {
                executor.submit(fetch_category_papers, category): category
                for category in self.categories
            }
            
            for future in as_completed(future_to_category):
                category = future_to_category[future]
                try:
                    papers = future.result()
                    all_papers.extend(papers)
                except Exception as exc:
                    logger.error(f"åˆ†ç±» {category} è·å–å¤±è´¥: {exc}")

        logger.success(f"è®ºæ–‡è·å–å®Œæˆ - æ€»è®¡: {len(all_papers)} ç¯‡")
        return all_papers

    def _process_single_paper(self, paper: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """å¤„ç†å•ç¯‡è®ºæ–‡ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶ã€‚"""
        max_retries = 2  # å‡å°‘é‡è¯•æ¬¡æ•°
        title_short = paper['title'][:50] + '...' if len(paper['title']) > 50 else paper['title']
        
        for attempt in range(max_retries):
            try:
                # æ·»åŠ è¯·æ±‚é—´éš”ï¼Œé¿å…APIé™æµ
                time.sleep(0.1)  # æ¯ç¯‡è®ºæ–‡è¯„ä¼°å‰ç­‰å¾…0.5ç§’
                
                # ç›´æ¥è°ƒç”¨è½»é‡æ¨¡å‹è¿›è¡Œç›¸å…³æ€§è¯„ä¼°ï¼ˆå†…è”ï¼Œå»é™¤é¢å¤–é—´æ¥å±‚ï¼‰
                evaluation = self.light_llm_provider.evaluate_paper_relevance(
                    paper, self.description, temperature=None
                )
                
                # åˆå¹¶è®ºæ–‡ä¿¡æ¯å’Œè¯„ä¼°ç»“æœ
                result = {
                    **paper,
                    **evaluation
                }
                
                logger.debug(f"è®ºæ–‡è¯„ä¼°å®Œæˆ - {title_short} (è¯„åˆ†: {evaluation['relevance_score']})")
                return result
                
            except Exception as e:
                logger.warning(f"è®ºæ–‡è¯„ä¼°å¤±è´¥ ({attempt + 1}/{max_retries}) - {title_short}: {e}")
                if attempt == max_retries - 1:
                    logger.error(f"è®ºæ–‡è¯„ä¼°å½»åº•å¤±è´¥ï¼Œè·³è¿‡ - {title_short}")
                    # å½“APIè°ƒç”¨å¤±è´¥æ—¶ï¼Œè¿”å›ä¸€ä¸ªæ ‡è®°ä»¥ä¾¿ä¸Šå±‚å¤„ç†
                    return {"__api_failed": True, "title": paper['title']}
                
                # æŒ‡æ•°é€€é¿é‡è¯•
                wait_time = (attempt + 1) * 2
                logger.debug(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•")
                time.sleep(wait_time)
        
        return None

    def get_recommendations(self, papers: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è·å–è®ºæ–‡æ¨èåˆ—è¡¨ã€‚"""
        logger.info(f"ç›¸å…³æ€§è¯„ä¼°å¼€å§‹ - å¾…è¯„ä¼°: {len(papers)} ç¯‡")
        
        recommended_papers = []
        api_failure_count = 0
        max_failures = 5  # æœ€å¤§å…è®¸å¤±è´¥æ¬¡æ•°
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†è®ºæ–‡ï¼Œé™ä½å¹¶å‘æ•°
        max_concurrent = min(self.num_workers, 2)  # æœ€å¤š2ä¸ªå¹¶å‘çº¿ç¨‹
        with ThreadPoolExecutor(max_workers=max_concurrent) as executor:
            future_to_paper = {
                executor.submit(self._process_single_paper, paper): paper
                for paper in papers
            }
            
            for future in as_completed(future_to_paper):
                paper = future_to_paper[future]
                try:
                    result = future.result()
                    if result:
                        # æ£€æŸ¥APIå¤±è´¥æ ‡è®°
                        if result.get("__api_failed"):
                            api_failure_count += 1
                            if api_failure_count >= max_failures:
                                logger.error(f"æ£€æµ‹åˆ°APIè°ƒç”¨å¤±è´¥è¾¾åˆ°ä¸Šé™({max_failures})ï¼Œç»ˆæ­¢è¯„ä¼°æµç¨‹")
                                raise Exception("APIè°ƒç”¨å¤±è´¥ï¼Œç»ˆæ­¢æµç¨‹")
                        else:
                            recommended_papers.append(result)
                except Exception as exc:
                    title_short = paper['title'][:50] + '...' if len(paper['title']) > 50 else paper['title']
                    logger.error(f"è®ºæ–‡å¤„ç†å¼‚å¸¸ - {title_short}: {exc}")
                    if "APIè°ƒç”¨å¤±è´¥" in str(exc):
                        raise  # é‡æ–°æŠ›å‡ºAPIå¤±è´¥å¼‚å¸¸

        # è¿‡æ»¤æ‰ç›¸å…³æ€§è¯„åˆ†ä½äº6åˆ†çš„è®ºæ–‡å’ŒAPIå¤±è´¥æ ‡è®°
        recommended_papers = [paper for paper in recommended_papers 
                          if paper.get('relevance_score', 0) >= 0 and not paper.get("__api_failed")]
        
        # æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åº
        recommended_papers.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        # é™åˆ¶æ¨èæ•°é‡ï¼ˆè¯¦ç»†åˆ†ææ•° + ç®€è¦åˆ†ææ•°ï¼‰
        max_total_papers = self.num_detailed_papers + self.num_brief_papers
        recommended_papers = recommended_papers[:max_total_papers]
        
        if api_failure_count > 0:
            logger.warning(f"ç›¸å…³æ€§è¯„ä¼°å®Œæˆ - æˆåŠŸ: {len(recommended_papers)} ç¯‡, APIå¤±è´¥: {api_failure_count} ç¯‡")
        else:
            logger.success(f"ç›¸å…³æ€§è¯„ä¼°å®Œæˆ - æ¨èè®ºæ–‡: {len(recommended_papers)} ç¯‡")
        return recommended_papers



    # ç§»é™¤æœªä½¿ç”¨çš„ summarize åŒ…è£…æ–¹æ³•ï¼šç›´æ¥ä½¿ç”¨ llm_provider.generate_summary_report

    def _process_single_paper_analysis(self, paper: Dict[str, Any]) -> str:
        """å¤„ç†å•ç¯‡è®ºæ–‡çš„è¯¦ç»†åˆ†æã€‚"""
        title_short = paper['title'][:50] + '...' if len(paper['title']) > 50 else paper['title']
        try:
            logger.debug(f"è·å–PDFå…¨æ–‡ - {title_short}")
            full_text = self.arxiv_fetcher.fetch_pdf_text(paper['pdf_url'])
            
            if "ä¸‹è½½PDFå¤±è´¥" in full_text or "å¤„ç†PDFå¤±è´¥" in full_text:
                logger.warning(f"PDFè·å–å¤±è´¥ï¼Œè·³è¿‡è¯¦ç»†åˆ†æ - {title_short}")
                return f"\n## {paper['title']}\n- **åˆ†æå¤±è´¥**: {full_text}\n"

            paper_with_full_text = {**paper, "full_text": full_text}
            
            logger.debug(f"ç”Ÿæˆè¯¦ç»†åˆ†æ - {title_short}")
            analysis = self.llm_provider.generate_detailed_paper_analysis(paper_with_full_text)
            logger.debug(f"è¯¦ç»†åˆ†æå®Œæˆ - {title_short}")
            return analysis
            
        except Exception as e:
            logger.error(f"è¯¦ç»†åˆ†æå¼‚å¸¸ - {title_short}: {e}")
            return f"\n## {paper['title']}\n- **åˆ†æå¤±è´¥**: {e}\n"

    def _generate_detailed_analysis(self, papers: List[Dict[str, Any]]) -> str:
        """ä¸ºè¯„åˆ†æœ€é«˜çš„å‡ ç¯‡è®ºæ–‡ç”Ÿæˆè¯¦ç»†åˆ†æã€‚"""
        if not papers or self.num_detailed_papers == 0:
            logger.debug("è·³è¿‡è¯¦ç»†åˆ†æ - æ— è®ºæ–‡æˆ–é…ç½®ä¸º0")
            return ""

        logger.info(f"è¯¦ç»†åˆ†æå¼€å§‹ - å‰ {self.num_detailed_papers} ç¯‡è®ºæ–‡")
        
        detailed_papers = papers[:self.num_detailed_papers]
        analysis_results = ["\n\n---\n\n# ğŸ“š è¯¦ç»†è®ºæ–‡åˆ—è¡¨\n"]

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†æ¯ç¯‡è®ºæ–‡
        with ThreadPoolExecutor(max_workers=min(self.num_workers, len(detailed_papers))) as executor:
            # æäº¤æ‰€æœ‰è®ºæ–‡å¤„ç†ä»»åŠ¡ï¼Œä¿æŒé¡ºåº
            futures = [executor.submit(self._process_single_paper_analysis, paper) 
                      for paper in detailed_papers]
            
            # æŒ‰åŸå§‹é¡ºåºæ”¶é›†ç»“æœï¼Œä¿æŒè®ºæ–‡æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åº
            for i, future in enumerate(futures):
                paper = detailed_papers[i]
                title_short = paper['title'][:50] + '...' if len(paper['title']) > 50 else paper['title']
                try:
                    analysis = future.result()
                    analysis_results.append(analysis)
                    # åœ¨æ¯ç¯‡è®ºæ–‡ä¹‹é—´æ·»åŠ åˆ†éš”çº¿ï¼ˆé™¤äº†æœ€åä¸€ç¯‡ï¼‰
                    if i < len(futures) - 1:
                        analysis_results.append("\n---\n")
                    logger.debug(f"è¯¦ç»†åˆ†æå®Œæˆ - {title_short}")
                except Exception as e:
                    logger.error(f"è¯¦ç»†åˆ†æä»»åŠ¡å¤±è´¥ - {title_short}: {e}")
                    analysis_results.append(f"\n## {paper['title']}\n- **åˆ†æå¤±è´¥**: ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}\n")
                    # åœ¨æ¯ç¯‡è®ºæ–‡ä¹‹é—´æ·»åŠ åˆ†éš”çº¿ï¼ˆé™¤äº†æœ€åä¸€ç¯‡ï¼‰
                    if i < len(futures) - 1:
                        analysis_results.append("\n---\n")

        logger.success(f"è¯¦ç»†åˆ†æå®Œæˆ - {len(detailed_papers)} ç¯‡")
        return "\n".join(analysis_results)

    def _generate_brief_analysis(self, papers: List[Dict[str, Any]]) -> str:
        """ä¸ºç¬¬num_detailed_papers+1åˆ°ç¬¬8ç¯‡è®ºæ–‡ç”Ÿæˆç®€è¦åˆ†æï¼ˆåŸºäºæ‘˜è¦çš„TLDRï¼‰ã€‚"""
        if not papers or len(papers) <= self.num_detailed_papers:
            return ""
        
        # è·å–éœ€è¦ç®€è¦åˆ†æçš„è®ºæ–‡ï¼ˆç¬¬num_detailed_papers+1åˆ°ç¬¬num_detailed_papers+num_brief_papersç¯‡ï¼‰
        start_idx = self.num_detailed_papers
        end_idx = min(self.num_detailed_papers + self.num_brief_papers, len(papers))
        brief_papers = papers[start_idx:end_idx]
        
        if not brief_papers:
            return ""
        
        logger.info(f"ç®€è¦åˆ†æå¼€å§‹ - ç¬¬ {start_idx+1} åˆ°ç¬¬ {end_idx} ç¯‡è®ºæ–‡")
        
        brief_results = ["\n\n---\n\n# ğŸ“ ç®€è¦è®ºæ–‡åˆ—è¡¨\n"]
        
        for i, paper in enumerate(brief_papers, start=start_idx+1):
            try:
                # ä½¿ç”¨LLMæä¾›å•†ç”Ÿæˆç®€è¦æ€»ç»“
                tldr = self.llm_provider.generate_brief_analysis(paper, temperature=None)
                
                # æ ¼å¼åŒ–è¾“å‡º
                brief_analysis = f"""
## {i}. {paper['title']}
- **ç›¸å…³æ€§è¯„åˆ†**: {'â­' * min(int(paper['relevance_score']), 5)} ({paper['relevance_score']}/10)
- **ArXiv ID**: {paper['arXiv_id']}
- **ä½œè€…**: {', '.join(paper['authors'])}
- **è®ºæ–‡é“¾æ¥**: <a href="{paper['pdf_url']}" class="link-btn pdf-link" target="_blank">PDF</a> <a href="{paper['abstract_url']}" class="link-btn arxiv-link" target="_blank">ArXiv</a>
- **TLDR**: {tldr.strip()}
""".strip()
                
                brief_results.append(brief_analysis)
                
                # åœ¨æ¯ç¯‡è®ºæ–‡ä¹‹é—´æ·»åŠ åˆ†éš”çº¿ï¼ˆé™¤äº†æœ€åä¸€ç¯‡ï¼‰
                if i < end_idx:
                    brief_results.append("\n---\n")
                    
                title_short = paper['title'][:50] + '...' if len(paper['title']) > 50 else paper['title']
                logger.debug(f"ç®€è¦åˆ†æå®Œæˆ - {title_short}")
                
            except Exception as e:
                title_short = paper['title'][:50] + '...' if len(paper['title']) > 50 else paper['title']
                logger.error(f"ç®€è¦åˆ†æå¤±è´¥ - {title_short}: {e}")
                brief_analysis = f"""
## {i}. {paper['title']}
- **ç›¸å…³æ€§è¯„åˆ†**: {'â­' * min(int(paper['relevance_score']), 5)} ({paper['relevance_score']}/10)
- **ArXiv ID**: {paper['arXiv_id']}
- **ä½œè€…**: {', '.join(paper['authors'])}
- **TLDR**: ç”Ÿæˆæ‘˜è¦å¤±è´¥
- **è®ºæ–‡é“¾æ¥**: <a href="{paper['pdf_url']}" class="link-btn pdf-link" target="_blank">PDF</a> <a href="{paper['abstract_url']}" class="link-btn arxiv-link" target="_blank">ArXiv</a>
""".strip()
                brief_results.append(brief_analysis)
                
                # åœ¨æ¯ç¯‡è®ºæ–‡ä¹‹é—´æ·»åŠ åˆ†éš”çº¿ï¼ˆé™¤äº†æœ€åä¸€ç¯‡ï¼‰
                if i < end_idx:
                    brief_results.append("\n---\n")
        
        logger.success(f"ç®€è¦åˆ†æå®Œæˆ - {len(brief_papers)} ç¯‡")
        return "\n".join(brief_results)

    def run(self, current_time: str, date: str = None) -> Optional[Dict[str, str]]:
        """è¿è¡Œå®Œæ•´çš„æ¨èæµç¨‹ã€‚
        
        Args:
            current_time: å½“å‰æ—¶é—´å­—ç¬¦ä¸²
            date: æŒ‡å®šæ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DDï¼Œå¦‚æœä¸ºNoneåˆ™è·å–æœ€æ–°è®ºæ–‡
            
        Returns:
            åŒ…å«summaryå’Œdetailed_analysisçš„å­—å…¸ï¼Œå¦‚æœæ²¡æœ‰æ¨èåˆ™è¿”å›None
        """
        logger.info("æ¨èå¼•æ“æµç¨‹å¼€å§‹")
        
        # 1. è·å–è®ºæ–‡
        papers = self._fetch_papers_from_categories(date)
        if not papers:
            logger.warning("è®ºæ–‡è·å–å¤±è´¥ - æœªè·å–åˆ°ä»»ä½•è®ºæ–‡ï¼Œæµç¨‹ç»ˆæ­¢")
            return None

        # 2. è·å–æ¨è
        recommended_papers = self.get_recommendations(papers)
        if not recommended_papers:
            logger.warning("æ¨èç”Ÿæˆå¤±è´¥ - æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡ï¼Œæµç¨‹ç»ˆæ­¢")
            return None
        
        # 3. ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘æ‰§è¡Œæ€»ç»“ç”Ÿæˆã€è¯¦ç»†åˆ†æå’Œç®€è¦åˆ†æ
        logger.info("å†…å®¹ç”Ÿæˆå¼€å§‹ - å¹¶å‘æ‰§è¡Œæ€»ç»“ã€è¯¦ç»†åˆ†æå’Œç®€è¦åˆ†æ")
        
        with ThreadPoolExecutor(max_workers=3) as executor:
            # æäº¤ä¸‰ä¸ªä»»åŠ¡åˆ°çº¿ç¨‹æ± ï¼ˆç›´æ¥ä½¿ç”¨llm_providerç”Ÿæˆæ€»ç»“æŠ¥å‘Šï¼Œç§»é™¤æ— ç”¨åŒ…è£…ï¼‰
            summary_future = executor.submit(self.llm_provider.generate_summary_report, recommended_papers, current_time)
            analysis_future = executor.submit(self._generate_detailed_analysis, recommended_papers)
            brief_future = executor.submit(self._generate_brief_analysis, recommended_papers)
            
            # ç­‰å¾…ä¸‰ä¸ªä»»åŠ¡å®Œæˆå¹¶è·å–ç»“æœ
            markdown_summary = summary_future.result()
            logger.debug("Markdownæ€»ç»“æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            
            detailed_analysis = analysis_future.result()
            logger.debug("è¯¦ç»†åˆ†æç”Ÿæˆå®Œæˆ")
            
            brief_analysis = brief_future.result()
            logger.debug("ç®€è¦åˆ†æç”Ÿæˆå®Œæˆ")
        
        # 4. è¿”å›åˆ†ç¦»çš„å†…å®¹è€Œä¸æ˜¯åˆå¹¶ï¼ŒåŒæ—¶åŒ…å«papersæ•°æ®
        result = {
            'summary': markdown_summary,
            'detailed_analysis': detailed_analysis,
            'brief_analysis': brief_analysis,
            'papers': recommended_papers  # æ·»åŠ papersæ•°æ®ç”¨äºç»Ÿè®¡
        }
        
        logger.success("æ¨èå¼•æ“æµç¨‹å®Œæˆ")
        return result


def main():
    """ç‹¬ç«‹æµ‹è¯•å‡½æ•°ã€‚"""
    import os
    from dotenv import load_dotenv
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv(os.path.join(os.path.dirname(os.path.dirname(__file__)), '.env'))
    
    # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
    api_key = os.getenv("DASHSCOPE_API_KEY")
    base_url = os.getenv("DASHSCOPE_BASE_URL")
    model = os.getenv("QWEN_MODEL")
    
    if not all([api_key, base_url, model]):
        logger.error("é”™è¯¯ï¼šè¯·ç¡®ä¿è®¾ç½®äº†DASHSCOPE_API_KEYã€DASHSCOPE_BASE_URLå’ŒQWEN_MODELç¯å¢ƒå˜é‡")
        return
    
    # è¯»å–ç ”ç©¶å…´è¶£æè¿°
    description_path = os.getenv("USER_CATEGORIES_FILE", "data/users/user_categories.json")
    try:
        import json
        with open(description_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        # è·å–ç¬¬ä¸€ä¸ªç”¨æˆ·çš„user_input
        if isinstance(data, list) and len(data) > 0:
            first_user = data[0]
            if isinstance(first_user, dict) and 'user_input' in first_user:
                description = first_user['user_input']
            else:
                logger.error(f"JSONæ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®ï¼Œç¼ºå°‘user_inputå­—æ®µ: {description_path}")
                return
        else:
            logger.error(f"JSONæ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®: {description_path}")
            return
    except FileNotFoundError:
        logger.error(f"æœªæ‰¾åˆ°ç”¨æˆ·åˆ†ç±»æ–‡ä»¶: {description_path}")
        return
    except json.JSONDecodeError as e:
        logger.error(f"JSONæ–‡ä»¶è§£æå¤±è´¥: {e}")
        return
    
    # åˆå§‹åŒ–æ¨èå¼•æ“
    categories = os.getenv("ARXIV_CATEGORIES", "cs.CL,cs.IR,cs.LG").split(',')
    max_entries = int(os.getenv("MAX_ENTRIES", "50"))
    num_detailed_papers = int(os.getenv("NUM_DETAILED_PAPERS", "3"))
    num_brief_papers = int(os.getenv("NUM_BRIEF_PAPERS", "7"))
    temperature = float(os.getenv("QWEN_MODEL_TEMPERATURE", "0.7"))
    top_p = float(os.getenv("QWEN_MODEL_TOP_P", "0.9"))
    max_tokens = int(os.getenv("QWEN_MODEL_MAX_TOKENS", "4000"))
    
    engine = RecommendationEngine(
        categories=categories,
        max_entries=max_entries,
        num_detailed_papers=num_detailed_papers,
        num_brief_papers=num_brief_papers,
        model=model,
        base_url=base_url,
        api_key=api_key,
        description=description,
        num_workers=int(os.getenv("MAX_WORKERS", "2")),
        temperature=temperature,
        top_p=top_p,
        max_tokens=max_tokens
    )
    
    # è¿è¡Œæ¨èæµç¨‹
    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    result = engine.run(current_time)
    
    if result:
        logger.success("æ¨èæµç¨‹æ‰§è¡ŒæˆåŠŸ")
        logger.debug(f"ç”Ÿæˆçš„HTMLå†…å®¹é•¿åº¦: {len(result)} å­—ç¬¦")
    else:
        logger.warning("æ¨èæµç¨‹æœªç”Ÿæˆå†…å®¹")


if __name__ == "__main__":
    main()
"""æ¨èå¼•æ“æ¨¡å—

æä¾›è®ºæ–‡æ¨èçš„æ ¸å¿ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬ä»ArXivè·å–è®ºæ–‡ã€ä½¿ç”¨LLMè¯„ä¼°ç›¸å…³æ€§ã€ç”Ÿæˆæ¨èæŠ¥å‘Šç­‰ã€‚
"""

import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any, Optional, Union
from datetime import datetime
from loguru import logger

from .arxiv_fetcher import ArxivFetcher
from .llm_provider import LLMProvider, create_light_llm_provider
from .pdf_text_extractor import PDFTextExtractor
from .progress_utils import ProgressTracker


class RecommendationEngine(ProgressTracker):
    """è®ºæ–‡æ¨èå¼•æ“ï¼Œè´Ÿè´£è·å–ã€è¯„ä¼°å’Œæ¨èArXivè®ºæ–‡ã€‚"""

    def __init__(
        self,
        categories: List[str],
        max_entries: int,
        num_detailed_papers: int,
        num_brief_papers: int,
        relevance_filter_threshold: int ,
        model: str,
        base_url: str,
        api_key: str,
        description: Union[str, Dict[str, str]],
        username: str = "TEST",
        num_workers: int = 2,
        temperature: float = 0.7,
        top_p: float = 0.9,
        max_tokens: int = 4000,
        arxiv_fetcher: Optional[ArxivFetcher] = None,
        llm_provider: Optional[LLMProvider] = None,
        light_llm_provider: Optional[LLMProvider] = None,
        pdf_text_extractor: Optional[PDFTextExtractor] = None,
        task_id: Optional[str] = None,
    ):
        """åˆå§‹åŒ–æ¨èå¼•æ“ã€‚
        
        Args:
            categories: ArXivåˆ†ç±»åˆ—è¡¨
            max_entries: æ¯ä¸ªåˆ†ç±»è·å–çš„æœ€å¤§è®ºæ–‡æ•°
            num_detailed_papers: è¯¦ç»†åˆ†æçš„è®ºæ–‡æ•°
            num_brief_papers: ç®€è¦åˆ†æçš„è®ºæ–‡æ•°
            model: LLMæ¨¡å‹åç§°
            base_url: LLM APIåŸºç¡€URL
            api_key: LLM APIå¯†é’¥
            description: ç ”ç©¶å…´è¶£æè¿°ï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²ï¼ˆå‘åå…¼å®¹ï¼‰æˆ–å­—å…¸æ ¼å¼
                         - å­—ç¬¦ä¸²æ ¼å¼ï¼šç›´æ¥ä½œä¸º positive_query
                         - å­—å…¸æ ¼å¼ï¼š{"positive_query": ..., "negative_query": ...}
            username: ç”¨æˆ·åï¼Œç”¨äºç”ŸæˆæŠ¥å‘Šæ—¶çš„ç½²å
            num_workers: å¹¶è¡Œå¤„ç†çº¿ç¨‹æ•°
            temperature: LLMç”Ÿæˆæ¸©åº¦
            top_p: LLM top_på‚æ•°
            max_tokens: LLMæœ€å¤§tokenæ•°
            task_id: ä»»åŠ¡IDï¼ˆç”¨äºè¿›åº¦æ›´æ–°ï¼‰
        """
        logger.info("æ¨èå¼•æ“åˆå§‹åŒ–å¼€å§‹")
        self.task_id = task_id
        
        # å‘åå…¼å®¹ï¼šå¦‚æœ description æ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        if isinstance(description, str):
            description_dict = {"positive_query": description, "negative_query": ""}
        else:
            description_dict = description
        
        self.categories = categories
        self.max_entries = max_entries
        self.num_detailed_papers = num_detailed_papers
        self.num_brief_papers = num_brief_papers
        self.description = description_dict  # å­˜å‚¨ä¸ºå­—å…¸æ ¼å¼
        self.num_workers = num_workers
        self.relevance_filter_threshold = relevance_filter_threshold
        
        # åˆå§‹åŒ–ArXivè·å–å™¨å’ŒLLMæä¾›å•†ï¼ˆæ”¯æŒä¾èµ–æ³¨å…¥ï¼Œå‡å°‘é‡å¤æ„é€ ä¸è€¦åˆï¼‰
        logger.debug("åˆå§‹åŒ–ArXivè·å–å™¨å’ŒLLMæä¾›å•†")
        self.arxiv_fetcher = arxiv_fetcher or ArxivFetcher()

        # ä¸»LLMæä¾›è€…ï¼ˆç”¨äºè¯¦ç»†åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆï¼‰
        # LLMProvider çš„ description å‚æ•°ä»ç„¶æ˜¯å­—ç¬¦ä¸²ï¼Œä½¿ç”¨ positive_query
        description_str = description_dict.get("positive_query", "")
        self.llm_provider = llm_provider or LLMProvider(
            model=model,
            base_url=base_url,
            api_key=api_key,
            description=description_str,
            username=username,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
        )

        # è½»é‡æ¨¡å‹æä¾›è€…ï¼ˆç”¨äºè®ºæ–‡ç›¸å…³æ€§è¯„ä¼°ï¼‰
        # create_light_llm_provider ä¹Ÿéœ€è¦å­—ç¬¦ä¸²æ ¼å¼
        self.light_llm_provider = light_llm_provider or create_light_llm_provider(
            description=description_str,
            username=username,
        )
        # PDF æ–‡æœ¬è§£æå™¨ï¼ˆç‹¬ç«‹æ¨¡å—ï¼Œå‡å°‘ ArxivFetcher èŒè´£ï¼‰
        self.pdf_text_extractor = pdf_text_extractor or PDFTextExtractor()
        
        self.temperature = temperature
        self.top_p = top_p
        self.max_tokens = max_tokens
        
        logger.success(f"æ¨èå¼•æ“åˆå§‹åŒ–å®Œæˆ - åˆ†ç±»: {categories}, è¯¦ç»†åˆ†æ: {num_detailed_papers}, ç®€è¦åˆ†æ: {num_brief_papers}")
        logger.debug(f"æ¨èå¼•æ“é…ç½® - num_detailed_papers={self.num_detailed_papers}, num_brief_papers={self.num_brief_papers}, max_total={self.num_detailed_papers + self.num_brief_papers}")
    
    # è¿›åº¦æ›´æ–°æ–¹æ³•å·²ä» ProgressTracker ç»§æ‰¿

    def _fetch_papers_from_categories(self, date: str = None) -> List[Dict[str, Any]]:
        """ä»æ‰€æœ‰æŒ‡å®šåˆ†ç±»ä¸­è·å–è®ºæ–‡ã€‚
        
        Args:
            date: æŒ‡å®šæ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DDï¼Œå¦‚æœä¸ºNoneåˆ™è·å–æœ€æ–°è®ºæ–‡
        """
        logger.info(f"è®ºæ–‡è·å–å¼€å§‹ - {len(self.categories)} ä¸ªåˆ†ç±»")
        all_papers = []
        
        def fetch_category_papers(category: str) -> List[Dict[str, Any]]:
            """è·å–å•ä¸ªåˆ†ç±»çš„è®ºæ–‡ã€‚"""
            logger.debug(f"è·å–åˆ†ç±» {category} çš„è®ºæ–‡")
            
            def on_progress(msg: str):
                self._update_progress(
                    log_message=msg
                )

            if date:
                # ä½¿ç”¨åŸºäºæ—¥æœŸçš„åˆ†é¡µè·å–
                papers = self.arxiv_fetcher.fetch_papers_paged(
                    category.strip(), 
                    date, 
                    per_page=min(self.max_entries, 200), 
                    max_pages=5,
                    max_total=self.max_entries,
                    progress_callback=on_progress
                )
                logger.debug(f"åˆ†ç±» {category} ({date}): {len(papers)} ç¯‡è®ºæ–‡")
            else:
                # ä½¿ç”¨åŸæœ‰çš„è·å–æ–¹å¼
                papers = self.arxiv_fetcher.fetch_papers(category.strip(), self.max_entries)
                logger.debug(f"åˆ†ç±» {category}: {len(papers)} ç¯‡è®ºæ–‡")
            return papers

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œè·å–ä¸åŒåˆ†ç±»çš„è®ºæ–‡
        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
            future_to_category = {
                executor.submit(fetch_category_papers, category): category
                for category in self.categories
            }
            
            for future in as_completed(future_to_category):
                category = future_to_category[future]
                try:
                    papers = future.result()
                    all_papers.extend(papers)
                except Exception as exc:
                    logger.error(f"åˆ†ç±» {category} è·å–å¤±è´¥: {exc}")

        logger.success(f"è®ºæ–‡è·å–å®Œæˆ - æ€»è®¡: {len(all_papers)} ç¯‡")
        return all_papers

    def _process_single_paper(self, paper: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """å¤„ç†å•ç¯‡è®ºæ–‡ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶ã€‚"""
        max_retries = 2  # å‡å°‘é‡è¯•æ¬¡æ•°
        title_short = paper['title'][:50] + '...' if len(paper['title']) > 50 else paper['title']
        
        for attempt in range(max_retries):
            try:
                # æ·»åŠ è¯·æ±‚é—´éš”ï¼Œé¿å…APIé™æµ
                time.sleep(0.1)  # æ¯ç¯‡è®ºæ–‡è¯„ä¼°å‰ç­‰å¾…0.5ç§’
                
                # ç›´æ¥è°ƒç”¨è½»é‡æ¨¡å‹è¿›è¡Œç›¸å…³æ€§è¯„ä¼°
                # å¿…é¡»è¦è®¾ç½® temperature=0 æ‰èƒ½å¾—åˆ°ç¨³å®šçš„ç»“æœ
                evaluation = self.light_llm_provider.evaluate_paper_relevance(
                    paper, self.description, temperature=0
                )
                
                # åˆå¹¶è®ºæ–‡ä¿¡æ¯å’Œè¯„ä¼°ç»“æœ
                result = {
                    **paper,
                    **evaluation
                }
                
                logger.debug(f"è®ºæ–‡è¯„ä¼°å®Œæˆ - {title_short} (è¯„åˆ†: {evaluation['relevance_score']})")
                return result
                
            except Exception as e:
                error_str = str(e).lower()
                error_type = type(e).__name__
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯è®¤è¯é”™è¯¯ï¼ˆAPIå¯†é’¥é”™è¯¯ï¼‰
                is_auth_error = (
                    any(keyword in error_str for keyword in ['unauthorized', '401', 'api_key', 'authentication', 'invalid_api_key']) or
                    'AuthenticationError' in error_type
                )
                
                if is_auth_error:
                    # è®¤è¯é”™è¯¯ï¼Œç«‹å³æŠ›å‡ºï¼Œä¸é‡è¯•
                    logger.error(f"APIè®¤è¯é”™è¯¯ï¼Œç»ˆæ­¢ä»»åŠ¡ - {title_short}: {e}")
                    raise Exception(f"APIè®¤è¯é”™è¯¯ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥é…ç½®: {e}")
                
                logger.warning(f"è®ºæ–‡è¯„ä¼°å¤±è´¥ ({attempt + 1}/{max_retries}) - {title_short}: {e}")
                if attempt == max_retries - 1:
                    logger.error(f"è®ºæ–‡è¯„ä¼°å½»åº•å¤±è´¥ï¼Œè·³è¿‡ - {title_short}")
                    # å½“APIè°ƒç”¨å¤±è´¥æ—¶ï¼Œè¿”å›ä¸€ä¸ªæ ‡è®°ä»¥ä¾¿ä¸Šå±‚å¤„ç†
                    return {"__api_failed": True, "title": paper['title']}
                
                # æŒ‡æ•°é€€é¿é‡è¯•
                wait_time = (attempt + 1) * 2
                logger.debug(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•")
                time.sleep(wait_time)
        
        return None

    def get_recommendations(self, papers: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è·å–è®ºæ–‡æ¨èåˆ—è¡¨ã€‚"""
        logger.info(f"ç›¸å…³æ€§è¯„ä¼°å¼€å§‹ - å¾…è¯„ä¼°: {len(papers)} ç¯‡")
        self._update_progress(
            step="è¯„ä¼°è®ºæ–‡ç›¸å…³æ€§...",
            percentage=30,
            log_message=f"å¼€å§‹è¯„ä¼° {len(papers)} ç¯‡è®ºæ–‡çš„ç›¸å…³æ€§"
        )
        
        recommended_papers = []
        api_failure_count = 0
        max_failures = 5  # æœ€å¤§å…è®¸å¤±è´¥æ¬¡æ•°
        total_papers = len(papers)
        processed_count = 0
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†è®ºæ–‡ï¼Œé™ä½å¹¶å‘æ•°
        max_concurrent = min(self.num_workers, 2)  # æœ€å¤š2ä¸ªå¹¶å‘çº¿ç¨‹
        with ThreadPoolExecutor(max_workers=max_concurrent) as executor:
            future_to_paper = {
                executor.submit(self._process_single_paper, paper): paper
                for paper in papers
            }
            
            for future in as_completed(future_to_paper):
                processed_count += 1
                # æ›´æ–°ç»†ç²’åº¦è¿›åº¦ (30-60%)
                progress_pct = 30 + int((processed_count / total_papers) * 30)
                self._update_progress(
                    step=f"è¯„ä¼°è®ºæ–‡ç›¸å…³æ€§... ({processed_count}/{total_papers})",
                    percentage=progress_pct,
                    log_message=f"å·²è¯„ä¼° {processed_count}/{total_papers} ç¯‡è®ºæ–‡"
                )
                paper = future_to_paper[future]
                try:
                    result = future.result()
                    if result:
                        # æ£€æŸ¥APIå¤±è´¥æ ‡è®°
                        if result.get("__api_failed"):
                            api_failure_count += 1
                            if api_failure_count >= max_failures:
                                error_msg = f"APIè°ƒç”¨å¤±è´¥è¾¾åˆ°ä¸Šé™({max_failures})ï¼Œç»ˆæ­¢è¯„ä¼°æµç¨‹"
                                logger.error(error_msg)
                                self._update_progress(
                                    step="è¯„ä¼°å¤±è´¥",
                                    percentage=0,
                                    log_message=error_msg,
                                    log_level="error"
                                )
                                raise Exception(error_msg)
                        else:
                            recommended_papers.append(result)
                except Exception as exc:
                    error_str = str(exc).lower()
                    # æ£€æŸ¥æ˜¯å¦æ˜¯è®¤è¯é”™è¯¯
                    is_auth_error = (
                        any(keyword in error_str for keyword in ['unauthorized', '401', 'api_key', 'authentication', 'invalid_api_key', 'apiè®¤è¯é”™è¯¯']) or
                        'AuthenticationError' in type(exc).__name__
                    )
                    
                    if is_auth_error:
                        # è®¤è¯é”™è¯¯ï¼Œç«‹å³ç»ˆæ­¢ä»»åŠ¡å¹¶æ›´æ–°è¿›åº¦
                        error_msg = f"APIè®¤è¯é”™è¯¯ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥é…ç½®: {exc}"
                        logger.error(error_msg)
                        self._update_progress(
                            step="APIè®¤è¯å¤±è´¥",
                            percentage=0,
                            log_message=error_msg,
                            log_level="error"
                        )
                        raise Exception(error_msg)
                    
                    title_short = paper['title'][:50] + '...' if len(paper['title']) > 50 else paper['title']
                    logger.error(f"è®ºæ–‡å¤„ç†å¼‚å¸¸ - {title_short}: {exc}")
                    if "APIè°ƒç”¨å¤±è´¥" in str(exc):
                        raise  # é‡æ–°æŠ›å‡ºAPIå¤±è´¥å¼‚å¸¸

        # è¿‡æ»¤æ‰ç›¸å…³æ€§è¯„åˆ†ä½äºé˜ˆå€¼çš„è®ºæ–‡å’ŒAPIå¤±è´¥æ ‡è®°ï¼ˆé˜ˆå€¼æ¥è‡ª .env é…ç½®ï¼‰ï¼Œå¹¶è¿›è¡Œ0â€“10èŒƒå›´è£å‰ª
        try:
            threshold_raw = int(float(self.relevance_filter_threshold))
        except Exception:
            threshold_raw = 0
        threshold = max(0, min(threshold_raw, 10))
        
        valid_papers_count = len([p for p in recommended_papers if not p.get("__api_failed")])
        recommended_papers = [
            paper for paper in recommended_papers
            if paper.get('relevance_score', 0) >= threshold and not paper.get("__api_failed")
        ]
        
        logger.info(f"ç›¸å…³æ€§è¿‡æ»¤å®Œæˆ - é˜ˆå€¼: {threshold}, è¿‡æ»¤å‰: {valid_papers_count}, è¿‡æ»¤å: {len(recommended_papers)}")
        if valid_papers_count > 0 and len(recommended_papers) == 0:
            self._update_progress(
                step="ç­›é€‰ç»“æœ",
                percentage=65,
                log_message=f"å·²å®Œæˆè¯„ä¼°ï¼Œ{valid_papers_count} ç¯‡å€™é€‰è®ºæ–‡å‡æœªè¾¾åˆ°ç›¸å…³æ€§é˜ˆå€¼({threshold})"
            )
        
        # æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åº
        recommended_papers.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        # é™åˆ¶æ¨èæ•°é‡ï¼ˆè¯¦ç»†åˆ†ææ•° + ç®€è¦åˆ†ææ•°ï¼‰
        max_total_papers = self.num_detailed_papers + self.num_brief_papers
        papers_before_limit = len(recommended_papers)
        logger.debug(f"è®ºæ–‡æ•°é‡é™åˆ¶ - è¯¦ç»†åˆ†æ: {self.num_detailed_papers}, ç®€è¦åˆ†æ: {self.num_brief_papers}, æœ€å¤§æ€»æ•°: {max_total_papers}, é™åˆ¶å‰è®ºæ–‡æ•°: {papers_before_limit}")
        recommended_papers = recommended_papers[:max_total_papers]
        
        if api_failure_count > 0:
            logger.warning(f"ç›¸å…³æ€§è¯„ä¼°å®Œæˆ - æˆåŠŸ: {len(recommended_papers)} ç¯‡, APIå¤±è´¥: {api_failure_count} ç¯‡")
        else:
            logger.success(f"ç›¸å…³æ€§è¯„ä¼°å®Œæˆ - æ¨èè®ºæ–‡: {len(recommended_papers)} ç¯‡ (é™åˆ¶å‰: {papers_before_limit} ç¯‡)")
        return recommended_papers



    # ç§»é™¤æœªä½¿ç”¨çš„ summarize åŒ…è£…æ–¹æ³•ï¼šç›´æ¥ä½¿ç”¨ llm_provider.generate_summary_report

    def _process_single_paper_analysis(self, paper: Dict[str, Any]) -> str:
        """å¤„ç†å•ç¯‡è®ºæ–‡çš„è¯¦ç»†åˆ†æã€‚"""
        title_short = paper['title'][:50] + '...' if len(paper['title']) > 50 else paper['title']
        try:
            logger.debug(f"è·å–PDFå…¨æ–‡ - {title_short}")
            full_text = self.pdf_text_extractor.extract_pdf_text(paper['pdf_url'])
            
            if "ä¸‹è½½PDFå¤±è´¥" in full_text or "å¤„ç†PDFå¤±è´¥" in full_text:
                logger.warning(f"PDFè·å–å¤±è´¥ï¼Œè·³è¿‡è¯¦ç»†åˆ†æ - {title_short}")
                return f"\n## {paper['title']}\n- **åˆ†æå¤±è´¥**: {full_text}\n"

            paper_with_full_text = {**paper, "full_text": full_text}
            
            logger.debug(f"ç”Ÿæˆè¯¦ç»†åˆ†æ - {title_short}")
            analysis = self.llm_provider.generate_detailed_paper_analysis(paper_with_full_text)
            logger.debug(f"è¯¦ç»†åˆ†æå®Œæˆ - {title_short}")
            return analysis
            
        except Exception as e:
            logger.error(f"è¯¦ç»†åˆ†æå¼‚å¸¸ - {title_short}: {e}")
            return f"\n## {paper['title']}\n- **åˆ†æå¤±è´¥**: {e}\n"

    def _generate_detailed_analysis(self, papers: List[Dict[str, Any]]) -> str:
        """ä¸ºè¯„åˆ†æœ€é«˜çš„å‡ ç¯‡è®ºæ–‡ç”Ÿæˆè¯¦ç»†åˆ†æã€‚"""
        if not papers or self.num_detailed_papers == 0:
            logger.debug("è·³è¿‡è¯¦ç»†åˆ†æ - æ— è®ºæ–‡æˆ–é…ç½®ä¸º0")
            return ""

        logger.info(f"è¯¦ç»†åˆ†æå¼€å§‹ - å‰ {self.num_detailed_papers} ç¯‡è®ºæ–‡")
        
        detailed_papers = papers[:self.num_detailed_papers]
        analysis_results = ["\n\n---\n\n# ğŸ“š è¯¦ç»†è®ºæ–‡åˆ—è¡¨\n"]

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†æ¯ç¯‡è®ºæ–‡
        with ThreadPoolExecutor(max_workers=min(self.num_workers, len(detailed_papers))) as executor:
            # æäº¤æ‰€æœ‰è®ºæ–‡å¤„ç†ä»»åŠ¡ï¼Œä¿æŒé¡ºåº
            futures = [executor.submit(self._process_single_paper_analysis, paper) 
                      for paper in detailed_papers]
            
            # æŒ‰åŸå§‹é¡ºåºæ”¶é›†ç»“æœï¼Œä¿æŒè®ºæ–‡æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åº
            for i, future in enumerate(futures):
                paper = detailed_papers[i]
                title_short = paper['title'][:50] + '...' if len(paper['title']) > 50 else paper['title']
                try:
                    analysis = future.result()
                    analysis_results.append(analysis)
                    # åœ¨æ¯ç¯‡è®ºæ–‡ä¹‹é—´æ·»åŠ åˆ†éš”çº¿ï¼ˆé™¤äº†æœ€åä¸€ç¯‡ï¼‰
                    if i < len(futures) - 1:
                        analysis_results.append("\n---\n")
                    logger.debug(f"è¯¦ç»†åˆ†æå®Œæˆ - {title_short}")
                except Exception as e:
                    logger.error(f"è¯¦ç»†åˆ†æä»»åŠ¡å¤±è´¥ - {title_short}: {e}")
                    analysis_results.append(f"\n## {paper['title']}\n- **åˆ†æå¤±è´¥**: ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}\n")
                    # åœ¨æ¯ç¯‡è®ºæ–‡ä¹‹é—´æ·»åŠ åˆ†éš”çº¿ï¼ˆé™¤äº†æœ€åä¸€ç¯‡ï¼‰
                    if i < len(futures) - 1:
                        analysis_results.append("\n---\n")

        logger.success(f"è¯¦ç»†åˆ†æå®Œæˆ - {len(detailed_papers)} ç¯‡")
        return "\n".join(analysis_results)

    def _generate_brief_analysis(self, papers: List[Dict[str, Any]]) -> str:
        """ä¸ºç¬¬num_detailed_papers+1åˆ°ç¬¬8ç¯‡è®ºæ–‡ç”Ÿæˆç®€è¦åˆ†æï¼ˆåŸºäºæ‘˜è¦çš„TLDRï¼‰ã€‚"""
        if not papers or len(papers) <= self.num_detailed_papers:
            return ""
        
        # è·å–éœ€è¦ç®€è¦åˆ†æçš„è®ºæ–‡ï¼ˆç¬¬num_detailed_papers+1åˆ°ç¬¬num_detailed_papers+num_brief_papersç¯‡ï¼‰
        start_idx = self.num_detailed_papers
        end_idx = min(self.num_detailed_papers + self.num_brief_papers, len(papers))
        brief_papers = papers[start_idx:end_idx]
        
        if not brief_papers:
            return ""
        
        logger.info(f"ç®€è¦åˆ†æå¼€å§‹ - ç¬¬ {start_idx+1} åˆ°ç¬¬ {end_idx} ç¯‡è®ºæ–‡")
        
        brief_results = ["\n\n---\n\n# ğŸ“ ç®€è¦è®ºæ–‡åˆ—è¡¨\n"]
        
        for i, paper in enumerate(brief_papers, start=start_idx+1):
            try:
                # ä½¿ç”¨LLMæä¾›å•†ç”Ÿæˆç®€è¦æ€»ç»“
                tldr = self.llm_provider.generate_brief_analysis(paper, temperature=None)
                
                # æ ¼å¼åŒ–è¾“å‡º
                brief_analysis = f"""
## {i}. {paper['title']}
- **ç›¸å…³æ€§è¯„åˆ†**: {'â­' * max(0, min(int(paper['relevance_score']), 10))} ({paper['relevance_score']}/10)
- **ArXiv ID**: {paper['arXiv_id']}
- **ä½œè€…**: {', '.join(paper['authors'])}
- **è®ºæ–‡é“¾æ¥**: <a href="{paper['pdf_url']}" class="link-btn pdf-link" target="_blank">PDF</a> <a href="{paper['abstract_url']}" class="link-btn arxiv-link" target="_blank">ArXiv</a>
- **TLDR**: {tldr.strip()}
""".strip()
                
                brief_results.append(brief_analysis)
                
                # åœ¨æ¯ç¯‡è®ºæ–‡ä¹‹é—´æ·»åŠ åˆ†éš”çº¿ï¼ˆé™¤äº†æœ€åä¸€ç¯‡ï¼‰
                if i < end_idx:
                    brief_results.append("\n---\n")
                    
                title_short = paper['title'][:50] + '...' if len(paper['title']) > 50 else paper['title']
                logger.debug(f"ç®€è¦åˆ†æå®Œæˆ - {title_short}")
                
            except Exception as e:
                title_short = paper['title'][:50] + '...' if len(paper['title']) > 50 else paper['title']
                logger.error(f"ç®€è¦åˆ†æå¤±è´¥ - {title_short}: {e}")
                brief_analysis = f"""
## {i}. {paper['title']}
- **ç›¸å…³æ€§è¯„åˆ†**: {'â­' * max(0, min(int(paper['relevance_score']), 10))} ({paper['relevance_score']}/10)
- **ArXiv ID**: {paper['arXiv_id']}
- **ä½œè€…**: {', '.join(paper['authors'])}
- **TLDR**: ç”Ÿæˆæ‘˜è¦å¤±è´¥
- **è®ºæ–‡é“¾æ¥**: <a href="{paper['pdf_url']}" class="link-btn pdf-link" target="_blank">PDF</a> <a href="{paper['abstract_url']}" class="link-btn arxiv-link" target="_blank">ArXiv</a>
""".strip()
                brief_results.append(brief_analysis)
                
                # åœ¨æ¯ç¯‡è®ºæ–‡ä¹‹é—´æ·»åŠ åˆ†éš”çº¿ï¼ˆé™¤äº†æœ€åä¸€ç¯‡ï¼‰
                if i < end_idx:
                    brief_results.append("\n---\n")
        
        logger.success(f"ç®€è¦åˆ†æå®Œæˆ - {len(brief_papers)} ç¯‡")
        return "\n".join(brief_results)

    def run(self, current_time: str, date: str = None) -> Optional[Dict[str, str]]:
        """è¿è¡Œå®Œæ•´çš„æ¨èæµç¨‹ã€‚
        
        Args:
            current_time: å½“å‰æ—¶é—´å­—ç¬¦ä¸²
            date: æŒ‡å®šæ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DDï¼Œå¦‚æœä¸ºNoneåˆ™è·å–æœ€æ–°è®ºæ–‡
            
        Returns:
            åŒ…å«summaryå’Œdetailed_analysisçš„å­—å…¸ï¼Œå¦‚æœæ²¡æœ‰æ¨èåˆ™è¿”å›None
        """
        logger.info("æ¨èå¼•æ“æµç¨‹å¼€å§‹")
        
        # 1. è·å–è®ºæ–‡
        self._update_progress(
            step="è·å–ArXivè®ºæ–‡...",
            percentage=15,
            log_message="æ­£åœ¨ä»ArXivè·å–è®ºæ–‡"
        )
        papers = self._fetch_papers_from_categories(date)
        if not papers:
            logger.warning("è®ºæ–‡è·å–å¤±è´¥ - æœªè·å–åˆ°ä»»ä½•è®ºæ–‡ï¼Œæµç¨‹ç»ˆæ­¢")
            self._update_progress(
                step="è®ºæ–‡è·å–å¤±è´¥",
                percentage=0,
                log_message="æœªè·å–åˆ°ä»»ä½•è®ºæ–‡",
                log_level="warning"
            )
            return None

        self._update_progress(
            step=f"æ£€ç´¢åˆ° {len(papers)} ç¯‡å€™é€‰",
            percentage=25,
            log_message=f"ä»ArXivæ£€ç´¢åˆ° {len(papers)} ç¯‡å€™é€‰è®ºæ–‡"
        )

        # 2. è·å–æ¨è
        try:
            recommended_papers = self.get_recommendations(papers)
        except Exception as e:
            # æ£€æŸ¥æ˜¯å¦æ˜¯è®¤è¯é”™è¯¯
            error_str = str(e).lower()
            is_auth_error = any(keyword in error_str for keyword in ['unauthorized', '401', 'api_key', 'authentication', 'invalid_api_key', 'apiè®¤è¯é”™è¯¯'])
            
            if is_auth_error:
                # è®¤è¯é”™è¯¯ï¼Œæ›´æ–°è¿›åº¦å¹¶é‡æ–°æŠ›å‡º
                error_msg = f"APIè®¤è¯é”™è¯¯ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥é…ç½®: {e}"
                logger.error(error_msg)
                self._update_progress(
                    step="APIè®¤è¯å¤±è´¥",
                    percentage=0,
                    log_message=error_msg,
                    log_level="error"
                )
                raise Exception(error_msg)
            else:
                # å…¶ä»–é”™è¯¯ï¼Œä¹Ÿæ›´æ–°è¿›åº¦å¹¶é‡æ–°æŠ›å‡º
                error_msg = f"è·å–æ¨èå¤±è´¥: {e}"
                logger.error(error_msg)
                self._update_progress(
                    step="æ¨èå¤±è´¥",
                    percentage=0,
                    log_message=error_msg,
                    log_level="error"
                )
                raise
        
        if not recommended_papers:
            logger.warning("æ¨èç”Ÿæˆå¤±è´¥ - ç»è¯„ä¼°æœªå‘ç°ç¬¦åˆå…´è¶£çš„è®ºæ–‡ï¼Œæµç¨‹ç»ˆæ­¢")
            self._update_progress(
                step="æœªå‘ç°ç›¸å…³è®ºæ–‡",
                percentage=0,
                log_message="ç»è¯„ä¼°æœªå‘ç°ç¬¦åˆå…´è¶£çš„è®ºæ–‡",
                log_level="warning"
            )
            return None
        
        self._update_progress(
            step=f"ç­›é€‰å‡º {len(recommended_papers)} ç¯‡ç›¸å…³è®ºæ–‡",
            percentage=60,
            log_message=f"å…±ç­›é€‰å‡º {len(recommended_papers)} ç¯‡ç›¸å…³è®ºæ–‡"
        )
        
        # 3. ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘æ‰§è¡Œæ€»ç»“ç”Ÿæˆã€è¯¦ç»†åˆ†æå’Œç®€è¦åˆ†æ
        logger.info("å†…å®¹ç”Ÿæˆå¼€å§‹ - å¹¶å‘æ‰§è¡Œæ€»ç»“ã€è¯¦ç»†åˆ†æå’Œç®€è¦åˆ†æ")
        self._update_progress(
            step="ç”ŸæˆæŠ¥å‘Šå†…å®¹...",
            percentage=65,
            log_message="å¼€å§‹ç”ŸæˆæŠ¥å‘Šå†…å®¹ï¼ˆæ€»ç»“ã€è¯¦ç»†åˆ†æã€ç®€è¦åˆ†æï¼‰"
        )
        
        with ThreadPoolExecutor(max_workers=3) as executor:
            # æäº¤ä¸‰ä¸ªä»»åŠ¡åˆ°çº¿ç¨‹æ± ï¼ˆç›´æ¥ä½¿ç”¨llm_providerç”Ÿæˆæ€»ç»“æŠ¥å‘Šï¼Œç§»é™¤æ— ç”¨åŒ…è£…ï¼‰
            # ä¼ é€’æœ€å¤§è®ºæ–‡æ•°é‡é™åˆ¶ï¼Œç¡®ä¿æ€»ç»“æŠ¥å‘Šä¹Ÿéµå®ˆç”¨æˆ·é…ç½®
            max_total_papers = self.num_detailed_papers + self.num_brief_papers
            summary_future = executor.submit(self.llm_provider.generate_summary_report, recommended_papers, current_time, None, max_total_papers)
            analysis_future = executor.submit(self._generate_detailed_analysis, recommended_papers)
            brief_future = executor.submit(self._generate_brief_analysis, recommended_papers)
            
            # ç­‰å¾…ä¸‰ä¸ªä»»åŠ¡å®Œæˆå¹¶è·å–ç»“æœ
            markdown_summary = summary_future.result()
            logger.debug("Markdownæ€»ç»“æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            self._update_progress(
                step="ç”ŸæˆæŠ¥å‘Šå†…å®¹... (1/3)",
                percentage=70,
                log_message="æ€»ç»“æŠ¥å‘Šç”Ÿæˆå®Œæˆ"
            )
            
            detailed_analysis = analysis_future.result()
            logger.debug("è¯¦ç»†åˆ†æç”Ÿæˆå®Œæˆ")
            self._update_progress(
                step="ç”ŸæˆæŠ¥å‘Šå†…å®¹... (2/3)",
                percentage=80,
                log_message="è¯¦ç»†åˆ†æç”Ÿæˆå®Œæˆ"
            )
            
            brief_analysis = brief_future.result()
            logger.debug("ç®€è¦åˆ†æç”Ÿæˆå®Œæˆ")
            self._update_progress(
                step="ç”ŸæˆæŠ¥å‘Šå†…å®¹... (3/3)",
                percentage=90,
                log_message="ç®€è¦åˆ†æç”Ÿæˆå®Œæˆ"
            )
        
        # 4. è¿”å›åˆ†ç¦»çš„å†…å®¹è€Œä¸æ˜¯åˆå¹¶ï¼ŒåŒæ—¶åŒ…å«papersæ•°æ®
        result = {
            'summary': markdown_summary,
            'detailed_analysis': detailed_analysis,
            'brief_analysis': brief_analysis,
            'papers': recommended_papers  # æ·»åŠ papersæ•°æ®ç”¨äºç»Ÿè®¡
        }
        
        self._update_progress(
            step="æŠ¥å‘Šç”Ÿæˆå®Œæˆ",
            percentage=95,
            log_message="æ¨èå¼•æ“æµç¨‹å®Œæˆï¼ŒæŠ¥å‘Šå·²ç”Ÿæˆ"
        )
        logger.success("æ¨èå¼•æ“æµç¨‹å®Œæˆ")
        return result


def main():
    """ç‹¬ç«‹æµ‹è¯•å‡½æ•°ã€‚"""
    import os
    from core.env_config import get_str, get_int, get_float, get_list
    
    # ä»é›†ä¸­åŒ–é…ç½®è·å–
    api_key = get_str("DASHSCOPE_API_KEY", "")
    base_url = get_str("DASHSCOPE_BASE_URL", "")
    model = get_str("QWEN_MODEL", "")
    
    if not all([api_key, base_url, model]):
        logger.error("é”™è¯¯ï¼šè¯·ç¡®ä¿è®¾ç½®äº†DASHSCOPE_API_KEYã€DASHSCOPE_BASE_URLå’ŒQWEN_MODELç¯å¢ƒå˜é‡")
        return
    
    # è¯»å–ç ”ç©¶å…´è¶£æè¿°ï¼ˆç¡¬ç¼–ç è·¯å¾„ï¼‰
    description_path = "data/users/user_categories.json"
    try:
        import json
        with open(description_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        # è·å–ç¬¬ä¸€ä¸ªç”¨æˆ·çš„user_inputã€negative_queryå’Œcategory_id
        if isinstance(data, list) and len(data) > 0:
            first_user = data[0]
            if isinstance(first_user, dict) and 'user_input' in first_user:
                positive_query = first_user['user_input']
                negative_query = first_user.get('negative_query', '')
                description = {
                    "positive_query": positive_query,
                    "negative_query": negative_query
                }
                
                # ä»ç”¨æˆ·é…ç½®æ–‡ä»¶è¯»å–åˆ†ç±»æ ‡ç­¾
                category_id = first_user.get('category_id', '')
                if category_id:
                    categories = [cat.strip() for cat in category_id.split(',') if cat.strip()]
                    if not categories:
                        logger.warning("category_idå­—æ®µä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®ï¼Œä½¿ç”¨é»˜è®¤åˆ†ç±»")
                        categories = ["cs.CL", "cs.IR", "cs.LG"]
                else:
                    logger.warning("ç”¨æˆ·é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰category_idå­—æ®µï¼Œä½¿ç”¨é»˜è®¤åˆ†ç±»")
                    categories = ["cs.CL", "cs.IR", "cs.LG"]
            else:
                logger.error(f"JSONæ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®ï¼Œç¼ºå°‘user_inputå­—æ®µ: {description_path}")
                return
        else:
            logger.error(f"JSONæ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®: {description_path}")
            return
    except FileNotFoundError:
        logger.error(f"æœªæ‰¾åˆ°ç”¨æˆ·åˆ†ç±»æ–‡ä»¶: {description_path}")
        return
    except json.JSONDecodeError as e:
        logger.error(f"JSONæ–‡ä»¶è§£æå¤±è´¥: {e}")
        return
    
    # åˆå§‹åŒ–æ¨èå¼•æ“ 
    max_entries = get_int("MAX_ENTRIES", 50)
    num_detailed_papers = get_int("NUM_DETAILED_PAPERS", 3)
    num_brief_papers = get_int("NUM_BRIEF_PAPERS", 7)
    temperature = get_float("QWEN_MODEL_TEMPERATURE", 0.7)
    top_p = get_float("QWEN_MODEL_TOP_P", 0.9)
    max_tokens = get_int("QWEN_MODEL_MAX_TOKENS", 4000)
    
    engine = RecommendationEngine(
        categories=categories,
        max_entries=max_entries,
        num_detailed_papers=num_detailed_papers,
        num_brief_papers=num_brief_papers,
        model=model,
        base_url=base_url,
        api_key=api_key,
        description=description,
        num_workers=get_int("MAX_WORKERS", 2),
        temperature=temperature,
        top_p=top_p,
        max_tokens=max_tokens
    )
    
    # è¿è¡Œæ¨èæµç¨‹
    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    result = engine.run(current_time)
    
    if result:
        logger.success("æ¨èæµç¨‹æ‰§è¡ŒæˆåŠŸ")
        logger.debug(f"ç”Ÿæˆçš„HTMLå†…å®¹é•¿åº¦: {len(result)} å­—ç¬¦")
    else:
        logger.warning("æ¨èæµç¨‹æœªç”Ÿæˆå†…å®¹")


if __name__ == "__main__":
    main()